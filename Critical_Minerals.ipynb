{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDDDBMwH4ZHI8hyIBRqtc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanguniyal-polcon/Machine-Learning/blob/main/Critical_Minerals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXcSR0MMZLNQ",
        "outputId": "3ac5a0cb-c49a-4df1-9dbd-061649c97472"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path1 = '/content/drive/MyDrive/critical_minerals/TradeData -FINAL.csv'         # Added .csv\n",
        "file_path2 = '/content/drive/MyDrive/critical_minerals/governance_data - FFFF.csv'    # Added .csv\n",
        "file_path3 = '/content/drive/MyDrive/critical_minerals/policy_buffers - FINAL.csv'     # Added .csv"
      ],
      "metadata": {
        "id": "Hg69pc7viI3U"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CRITICAL MINERALS SUPPLY CHAIN VULNERABILITY PREDICTION\")\n",
        "print(\"Machine Learning Framework - Using Real Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD YOUR REAL DATA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: Loading Your Data Files\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\n",
        "try:\n",
        "    # Load your three CSV files\n",
        "    trade_df = pd.read_csv(file_path1)\n",
        "    governance_df = pd.read_csv(file_path2)\n",
        "    policy_df = pd.read_csv(file_path3)\n",
        "\n",
        "    print(\"‚úì Successfully loaded all three files!\")\n",
        "    print(f\"\\n  trade_data.csv: {len(trade_df)} rows\")\n",
        "    print(f\"  governance_data.csv: {len(governance_df)} rows\")\n",
        "    print(f\"  policy_buffers.csv: {len(policy_df)} rows\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: Could not find file: {e}\")\n",
        "    print(\"\\nPlease ensure you've uploaded:\")\n",
        "    print(\"  1. trade_data.csv\")\n",
        "    print(\"  2. governance_data.csv\")\n",
        "    print(\"  3. policy_buffers.csv\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "oocmqvznrZUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e773794b-869a-4019-d220-e49879612608"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CRITICAL MINERALS SUPPLY CHAIN VULNERABILITY PREDICTION\n",
            "Machine Learning Framework - Using Real Data\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Loading Your Data Files\n",
            "--------------------------------------------------------------------------------\n",
            "‚úì Successfully loaded all three files!\n",
            "\n",
            "  trade_data.csv: 316 rows\n",
            "  governance_data.csv: 840 rows\n",
            "  policy_buffers.csv: 840 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FIX: CONVERT STRING COLUMNS TO NUMERIC\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONVERTING DATA TYPES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Function to clean and convert numeric columns\n",
        "def clean_numeric(df, columns):\n",
        "    \"\"\"Remove commas, dollar signs, and convert to numeric\"\"\"\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            # Convert to string first (in case already numeric)\n",
        "            df[col] = df[col].astype(str)\n",
        "\n",
        "            # Remove commas, dollar signs, and other characters\n",
        "            df[col] = df[col].str.replace(',', '', regex=False)\n",
        "            df[col] = df[col].str.replace('$', '', regex=False)\n",
        "            df[col] = df[col].str.replace('‚Çπ', '', regex=False)\n",
        "            df[col] = df[col].str.strip()\n",
        "\n",
        "            # Convert to numeric (coerce errors to NaN)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            print(f\"‚úì Converted {col} to numeric (type: {df[col].dtype})\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Clean trade_data numeric columns\n",
        "print(\"\\nCleaning trade_data.csv...\")\n",
        "trade_df = clean_numeric(trade_df, [\n",
        "    'year',\n",
        "    'import_value_usd',\n",
        "    'import_tonnage'\n",
        "])\n",
        "\n",
        "# Clean governance_data numeric columns\n",
        "print(\"\\nCleaning governance_data.csv...\")\n",
        "governance_df = clean_numeric(governance_df, [\n",
        "    'year',\n",
        "    'political_stability',\n",
        "    'regulatory_quality',\n",
        "    'control_of_corruption'\n",
        "])\n",
        "\n",
        "# Clean policy_buffers numeric columns\n",
        "print(\"\\nCleaning policy_buffers.csv...\")\n",
        "policy_df = clean_numeric(policy_df, [\n",
        "    'year',\n",
        "    'fta_active',\n",
        "    'mou_minerals'\n",
        "])\n",
        "\n",
        "# Check for NaN values introduced by conversion\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"CHECKING FOR CONVERSION ISSUES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "trade_nan = trade_df[['import_value_usd', 'import_tonnage']].isna().sum()\n",
        "if trade_nan.sum() > 0:\n",
        "    print(f\"\\n‚ö† WARNING: NaN values in trade_data after conversion:\")\n",
        "    print(trade_nan[trade_nan > 0])\n",
        "    print(\"\\nShowing problematic rows:\")\n",
        "    print(trade_df[trade_df['import_tonnage'].isna()][['year', 'mineral', 'supplier_country', 'import_value_usd', 'import_tonnage']])\n",
        "else:\n",
        "    print(\"\\n‚úì trade_data: All numeric columns converted successfully\")\n",
        "\n",
        "gov_nan = governance_df[['political_stability', 'regulatory_quality', 'control_of_corruption']].isna().sum()\n",
        "if gov_nan.sum() > 0:\n",
        "    print(f\"\\n‚ö† WARNING: NaN values in governance_data after conversion:\")\n",
        "    print(gov_nan[gov_nan > 0])\n",
        "else:\n",
        "    print(\"‚úì governance_data: All numeric columns converted successfully\")\n",
        "\n",
        "policy_nan = policy_df[['fta_active', 'mou_minerals']].isna().sum()\n",
        "if policy_nan.sum() > 0:\n",
        "    print(f\"\\n‚ö† WARNING: NaN values in policy_buffers after conversion:\")\n",
        "    print(policy_nan[policy_nan > 0])\n",
        "else:\n",
        "    print(\"‚úì policy_buffers: All numeric columns converted successfully\")\n",
        "\n",
        "# Display data types\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"FINAL DATA TYPES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\ntrade_data.csv:\")\n",
        "print(trade_df.dtypes)\n",
        "\n",
        "print(\"\\ngovernance_data.csv:\")\n",
        "print(governance_df.dtypes)\n",
        "\n",
        "print(\"\\npolicy_buffers.csv:\")\n",
        "print(policy_df.dtypes)\n",
        "\n",
        "print(\"\\n‚úì Data type conversion complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j80xL3SBndro",
        "outputId": "2e4bdc3a-6e5a-4565-a255-67221d624635"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CONVERTING DATA TYPES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaning trade_data.csv...\n",
            "‚úì Converted year to numeric (type: int64)\n",
            "‚úì Converted import_value_usd to numeric (type: float64)\n",
            "‚úì Converted import_tonnage to numeric (type: float64)\n",
            "\n",
            "Cleaning governance_data.csv...\n",
            "‚úì Converted year to numeric (type: int64)\n",
            "‚úì Converted political_stability to numeric (type: float64)\n",
            "‚úì Converted regulatory_quality to numeric (type: float64)\n",
            "‚úì Converted control_of_corruption to numeric (type: float64)\n",
            "\n",
            "Cleaning policy_buffers.csv...\n",
            "‚úì Converted year to numeric (type: int64)\n",
            "‚úì Converted fta_active to numeric (type: int64)\n",
            "‚úì Converted mou_minerals to numeric (type: int64)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "CHECKING FOR CONVERSION ISSUES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚úì trade_data: All numeric columns converted successfully\n",
            "‚úì governance_data: All numeric columns converted successfully\n",
            "‚úì policy_buffers: All numeric columns converted successfully\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "FINAL DATA TYPES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "trade_data.csv:\n",
            "year                  int64\n",
            "mineral              object\n",
            "supplier_country     object\n",
            "import_value_usd    float64\n",
            "import_tonnage      float64\n",
            "dtype: object\n",
            "\n",
            "governance_data.csv:\n",
            "year                       int64\n",
            "country                   object\n",
            "political_stability      float64\n",
            "regulatory_quality       float64\n",
            "control_of_corruption    float64\n",
            "dtype: object\n",
            "\n",
            "policy_buffers.csv:\n",
            "year             int64\n",
            "country         object\n",
            "fta_active       int64\n",
            "mou_minerals     int64\n",
            "dtype: object\n",
            "\n",
            "‚úì Data type conversion complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: DATA VALIDATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Validating Data Structure\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Validate trade_data columns\n",
        "required_trade_cols = ['year', 'mineral', 'supplier_country', 'import_value_usd', 'import_tonnage']\n",
        "if not all(col in trade_df.columns for col in required_trade_cols):\n",
        "    print(f\"‚ùå ERROR: trade_data.csv missing required columns\")\n",
        "    print(f\"   Required: {required_trade_cols}\")\n",
        "    print(f\"   Found: {list(trade_df.columns)}\")\n",
        "    raise ValueError(\"Incorrect trade_data.csv column names\")\n",
        "\n",
        "# Validate governance_data columns\n",
        "required_gov_cols = ['year', 'country', 'political_stability', 'regulatory_quality', 'control_of_corruption']\n",
        "if not all(col in governance_df.columns for col in required_gov_cols):\n",
        "    print(f\"‚ùå ERROR: governance_data.csv missing required columns\")\n",
        "    print(f\"   Required: {required_gov_cols}\")\n",
        "    print(f\"   Found: {list(governance_df.columns)}\")\n",
        "    raise ValueError(\"Incorrect governance_data.csv column names\")\n",
        "\n",
        "# Validate policy_buffers columns\n",
        "required_policy_cols = ['year', 'country', 'fta_active', 'mou_minerals']\n",
        "if not all(col in policy_df.columns for col in required_policy_cols):\n",
        "    print(f\"‚ùå ERROR: policy_buffers.csv missing required columns\")\n",
        "    print(f\"   Required: {required_policy_cols}\")\n",
        "    print(f\"   Found: {list(policy_df.columns)}\")\n",
        "    raise ValueError(\"Incorrect policy_buffers.csv column names\")\n",
        "\n",
        "print(\"‚úì All column names are correct!\")\n",
        "\n",
        "# Check for missing values\n",
        "trade_missing = trade_df.isnull().sum().sum()\n",
        "gov_missing = governance_df.isnull().sum().sum()\n",
        "policy_missing = policy_df.isnull().sum().sum()\n",
        "\n",
        "if trade_missing > 0:\n",
        "    print(f\"\\n‚ö† WARNING: trade_data.csv has {trade_missing} missing values\")\n",
        "    print(trade_df.isnull().sum()[trade_df.isnull().sum() > 0])\n",
        "\n",
        "if gov_missing > 0:\n",
        "    print(f\"\\n‚ö† WARNING: governance_data.csv has {gov_missing} missing values\")\n",
        "    print(governance_df.isnull().sum()[governance_df.isnull().sum() > 0])\n",
        "\n",
        "if policy_missing > 0:\n",
        "    print(f\"\\n‚ö† WARNING: policy_buffers.csv has {policy_missing} missing values\")\n",
        "    print(policy_df.isnull().sum()[policy_df.isnull().sum() > 0])\n",
        "\n",
        "# Check country name consistency\n",
        "trade_countries = set(trade_df['supplier_country'].unique())\n",
        "gov_countries = set(governance_df['country'].unique())\n",
        "policy_countries = set(policy_df['country'].unique())\n",
        "\n",
        "missing_in_gov = trade_countries - gov_countries\n",
        "missing_in_policy = trade_countries - policy_countries\n",
        "\n",
        "if missing_in_gov:\n",
        "    print(f\"\\n‚ö† WARNING: Countries in trade_data but missing in governance_data:\")\n",
        "    for country in missing_in_gov:\n",
        "        print(f\"   - {country}\")\n",
        "    print(\"   Fix: Add these countries to governance_data.csv\")\n",
        "\n",
        "if missing_in_policy:\n",
        "    print(f\"\\n‚ö† WARNING: Countries in trade_data but missing in policy_buffers:\")\n",
        "    for country in missing_in_policy:\n",
        "        print(f\"   - {country}\")\n",
        "    print(\"   Fix: Add these countries to policy_buffers.csv\")\n",
        "\n",
        "if not missing_in_gov and not missing_in_policy and trade_missing == 0 and gov_missing == 0 and policy_missing == 0:\n",
        "    print(\"\\n‚úì Data validation passed! All checks successful.\")\n",
        "\n",
        "# Display data summary\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"DATA SUMMARY\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\nTrade Data:\")\n",
        "print(f\"  Years: {trade_df['year'].min()} to {trade_df['year'].max()}\")\n",
        "print(f\"  Minerals: {trade_df['mineral'].nunique()} unique ({', '.join(trade_df['mineral'].unique())})\")\n",
        "print(f\"  Countries: {trade_df['supplier_country'].nunique()} unique\")\n",
        "print(f\"  Total observations: {len(trade_df)}\")\n",
        "\n",
        "print(f\"\\nGovernance Data:\")\n",
        "print(f\"  Years: {governance_df['year'].min()} to {governance_df['year'].max()}\")\n",
        "print(f\"  Countries: {governance_df['country'].nunique()} unique\")\n",
        "\n",
        "print(f\"\\nPolicy Data:\")\n",
        "print(f\"  Countries with active FTAs: {policy_df[policy_df['fta_active']==1]['country'].nunique()}\")\n",
        "print(f\"  Countries with minerals MoUs: {policy_df[policy_df['mou_minerals']==1]['country'].nunique()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN-oMTqqmjzb",
        "outputId": "f6dd6c79-516a-4b2c-8d8b-86e8ba95ab52"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: Validating Data Structure\n",
            "--------------------------------------------------------------------------------\n",
            "‚úì All column names are correct!\n",
            "\n",
            "‚úì Data validation passed! All checks successful.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATA SUMMARY\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Trade Data:\n",
            "  Years: 2005 to 2024\n",
            "  Minerals: 2 unique (lithium, cobalt)\n",
            "  Countries: 42 unique\n",
            "  Total observations: 316\n",
            "\n",
            "Governance Data:\n",
            "  Years: 2005 to 2024\n",
            "  Countries: 42 unique\n",
            "\n",
            "Policy Data:\n",
            "  Countries with active FTAs: 9\n",
            "  Countries with minerals MoUs: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: MERGE DATA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Merging Datasets\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Merge trade with governance\n",
        "print(\"\\nMerging trade data with governance indicators...\")\n",
        "combined_df = trade_df.merge(\n",
        "    governance_df,\n",
        "    left_on=['year', 'supplier_country'],\n",
        "    right_on=['year', 'country'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Check merge success\n",
        "unmatched_gov = combined_df['political_stability'].isna().sum()\n",
        "if unmatched_gov > 0:\n",
        "    print(f\"‚ö† WARNING: {unmatched_gov} trade rows did not match governance data\")\n",
        "    print(\"   This may cause issues. Check country name spelling consistency.\")\n",
        "else:\n",
        "    print(f\"‚úì All {len(combined_df)} rows matched with governance data\")\n",
        "\n",
        "# Merge with policy buffers\n",
        "print(\"\\nMerging with policy buffers...\")\n",
        "combined_df = combined_df.merge(\n",
        "    policy_df,\n",
        "    left_on=['year', 'supplier_country'],\n",
        "    right_on=['year', 'country'],\n",
        "    how='left',\n",
        "    suffixes=('', '_policy')\n",
        ")\n",
        "\n",
        "# Check merge success\n",
        "unmatched_policy = combined_df['fta_active'].isna().sum()\n",
        "if unmatched_policy > 0:\n",
        "    print(f\"‚ö† WARNING: {unmatched_policy} rows did not match policy data\")\n",
        "    # Fill missing policy values with 0 (assume no FTA/MoU if missing)\n",
        "    combined_df['fta_active'] = combined_df['fta_active'].fillna(0)\n",
        "    combined_df['mou_minerals'] = combined_df['mou_minerals'].fillna(0)\n",
        "else:\n",
        "    print(f\"‚úì All rows matched with policy data\")\n",
        "\n",
        "# Clean up duplicate country columns\n",
        "if 'country_policy' in combined_df.columns:\n",
        "    combined_df = combined_df.drop(columns=['country_policy'])\n",
        "\n",
        "print(f\"\\n‚úì Final combined dataset: {len(combined_df)} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV3Em2uDmqSh",
        "outputId": "68cc5715-a95e-4c8f-c3ba-99074b0fc7dc"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: Merging Datasets\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Merging trade data with governance indicators...\n",
            "‚úì All 316 rows matched with governance data\n",
            "\n",
            "Merging with policy buffers...\n",
            "‚ö† WARNING: 1 rows did not match policy data\n",
            "\n",
            "‚úì Final combined dataset: 317 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Engineering Features\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: What is feature engineering?\n",
        "- Creating new variables (features) from raw data\n",
        "- These help the model learn patterns better\n",
        "- Example: Converting raw tonnage into \"concentration\" metric\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCreating trade concentration metrics...\")\n",
        "\n",
        "# Calculate total annual imports per mineral\n",
        "annual_totals = combined_df.groupby(['year', 'mineral'])['import_tonnage'].transform('sum')\n",
        "combined_df['total_annual_imports'] = annual_totals\n",
        "\n",
        "# Import concentration (supplier's share of total imports)\n",
        "combined_df['import_concentration'] = combined_df['import_tonnage'] / combined_df['total_annual_imports']\n",
        "\n",
        "# HHI (Herfindahl-Hirschman Index) - market concentration measure\n",
        "hhi_data = combined_df.groupby(['year', 'mineral']).apply(\n",
        "    lambda x: (x['import_concentration'] ** 2).sum()\n",
        ").reset_index(name='hhi')\n",
        "\n",
        "combined_df = combined_df.merge(hhi_data, on=['year', 'mineral'], how='left')\n",
        "\n",
        "# Supplier market dominance (1 if concentration > 50%, else 0)\n",
        "combined_df['supplier_market_dominance'] = (combined_df['import_concentration'] > 0.5).astype(int)\n",
        "\n",
        "print(\"‚úì Created concentration metrics: import_concentration, hhi, supplier_market_dominance\")\n",
        "\n",
        "# Create lagged features for time-series patterns\n",
        "print(\"\\nCreating time-series features (lagged values)...\")\n",
        "\n",
        "combined_df = combined_df.sort_values(['mineral', 'supplier_country', 'year'])\n",
        "\n",
        "# Lag tonnage by 1 year\n",
        "combined_df['import_tonnage_lag'] = combined_df.groupby(['mineral', 'supplier_country'])['import_tonnage'].shift(1)\n",
        "\n",
        "# Lag value by 1 year\n",
        "combined_df['import_value_lag'] = combined_df.groupby(['mineral', 'supplier_country'])['import_value_usd'].shift(1)\n",
        "\n",
        "print(\"‚úì Created lagged features: import_tonnage_lag, import_value_lag\")\n",
        "\n",
        "# Calculate percentage changes\n",
        "print(\"\\nCalculating percentage changes...\")\n",
        "\n",
        "epsilon = 1.0  # Small value to avoid division by zero\n",
        "\n",
        "combined_df['tonnage_pct_change'] = (\n",
        "    (combined_df['import_tonnage'] - combined_df['import_tonnage_lag']) /\n",
        "    (combined_df['import_tonnage_lag'] + epsilon) * 100\n",
        ").clip(-100, 1000)  # Cap extreme values\n",
        "\n",
        "combined_df['value_pct_change'] = (\n",
        "    (combined_df['import_value_usd'] - combined_df['import_value_lag']) /\n",
        "    (combined_df['import_value_lag'] + epsilon) * 100\n",
        ").clip(-100, 1000)\n",
        "\n",
        "print(\"‚úì Created change metrics: tonnage_pct_change, value_pct_change\")\n",
        "\n",
        "# Price volatility\n",
        "print(\"\\nCalculating price volatility...\")\n",
        "\n",
        "combined_df['price_per_tonne'] = np.where(\n",
        "    combined_df['import_tonnage'] > 0,\n",
        "    combined_df['import_value_usd'] / combined_df['import_tonnage'],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# Fill NaN prices with mineral-year median\n",
        "combined_df['price_per_tonne'] = combined_df.groupby(['mineral', 'year'])['price_per_tonne'].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "# Price volatility (rolling standard deviation)\n",
        "combined_df['price_volatility'] = combined_df.groupby(['mineral', 'supplier_country'])['price_per_tonne'].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).std()\n",
        ")\n",
        "\n",
        "print(\"‚úì Created price metrics: price_per_tonne, price_volatility\")\n",
        "\n",
        "# --- NEW FEATURES ---\n",
        "# Economic features\n",
        "combined_df['price_tonnage_ratio'] = combined_df['import_value_usd'] / combined_df['import_tonnage']\n",
        "\n",
        "# NOTE: Since you already have a 'price_per_tonne' calculation in your notebook,\n",
        "# use that existing column to calculate the percentage change:\n",
        "combined_df['price_change_pct'] = combined_df.groupby(['mineral', 'supplier_country'])['price_per_tonne'].pct_change() * 100\n",
        "\n",
        "# Governance composite\n",
        "combined_df['governance_risk_score'] = (\n",
        "    (2.5 - combined_df['political_stability']) +\n",
        "    (2.5 - combined_df['regulatory_quality']) +\n",
        "    (2.5 - combined_df['control_of_corruption'])\n",
        ") / 7.5\n",
        "\n",
        "# Supply chain features\n",
        "combined_df['supplier_concentration_squared'] = combined_df['import_concentration'] ** 2\n",
        "combined_df['high_concentration_flag'] = (combined_df['import_concentration'] > 0.3).astype(int)\n",
        "\n",
        "# Interaction features\n",
        "combined_df['risk_weighted_volume'] = combined_df['import_tonnage'] * (1 - (combined_df['political_stability'] + 2.5) / 5)\n",
        "\n",
        "# Drop rows with NaN in lagged features (first year for each supplier)\n",
        "rows_before = len(combined_df)\n",
        "combined_df = combined_df.dropna(subset=['import_tonnage_lag'])\n",
        "rows_after = len(combined_df)\n",
        "print(f\"\\n‚úì Dropped {rows_before - rows_after} rows with missing lagged values\")\n",
        "print(f\"  Final dataset: {rows_after} observations\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LPge3Oi4mx_N",
        "outputId": "2e9424bf-5184-41a1-9bbd-ecfc7f16afb4"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: Engineering Features\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Creating trade concentration metrics...\n",
            "‚úì Created concentration metrics: import_concentration, hhi, supplier_market_dominance\n",
            "\n",
            "Creating time-series features (lagged values)...\n",
            "‚úì Created lagged features: import_tonnage_lag, import_value_lag\n",
            "\n",
            "Calculating percentage changes...\n",
            "‚úì Created change metrics: tonnage_pct_change, value_pct_change\n",
            "\n",
            "Calculating price volatility...\n",
            "‚úì Created price metrics: price_per_tonne, price_volatility\n",
            "\n",
            "‚úì Dropped 60 rows with missing lagged values\n",
            "  Final dataset: 257 observations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: CREATE TARGET VARIABLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: Creating Target Variable (Supply Fragility Events)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: What is a target variable?\n",
        "- The outcome we want to predict\n",
        "- Binary: 0 = stable supply, 1 = fragility event\n",
        "- Fragility event = significant supply disruption\n",
        "\n",
        "Definition:\n",
        "- Volume drop > 20% YoY, OR\n",
        "- Price spike > 90th percentile volatility\n",
        "\"\"\"\n",
        "\n",
        "# Calculate thresholds\n",
        "price_volatility_90th = combined_df['price_volatility'].quantile(0.90)\n",
        "\n",
        "print(f\"\\nThresholds:\")\n",
        "print(f\"  Volume drop: >20% decrease\")\n",
        "print(f\"  Price spike: >{price_volatility_90th:.2f} (90th percentile)\")\n",
        "\n",
        "# Define fragility events\n",
        "combined_df['supply_fragility_event'] = (\n",
        "    (combined_df['tonnage_pct_change'] < -20) |  # Volume drop\n",
        "    (combined_df['price_volatility'] > price_volatility_90th)  # Price spike\n",
        ").astype(int)\n",
        "\n",
        "# Count events\n",
        "fragility_count = combined_df['supply_fragility_event'].sum()\n",
        "fragility_pct = fragility_count / len(combined_df) * 100\n",
        "\n",
        "print(f\"\\nFragility Events:\")\n",
        "print(f\"  Total events: {fragility_count}\")\n",
        "print(f\"  Percentage: {fragility_pct:.1f}%\")\n",
        "print(f\"  Stable periods: {len(combined_df) - fragility_count} ({100-fragility_pct:.1f}%)\")\n",
        "\n",
        "if fragility_count < 10:\n",
        "    print(f\"\\n‚ö† WARNING: Only {fragility_count} fragility events detected\")\n",
        "    print(\"   This is very low and may affect model training.\")\n",
        "    print(\"   Consider:\")\n",
        "    print(\"   - Lowering thresholds (e.g., -15% volume drop)\")\n",
        "    print(\"   - Adding more years of data\")\n",
        "    print(\"   - Including more minerals\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXoh2Bd7nvvH",
        "outputId": "2582f380-3aa9-4fa8-c9ab-93149731a36c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: Creating Target Variable (Supply Fragility Events)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Thresholds:\n",
            "  Volume drop: >20% decrease\n",
            "  Price spike: >16682.85 (90th percentile)\n",
            "\n",
            "Fragility Events:\n",
            "  Total events: 120\n",
            "  Percentage: 46.7%\n",
            "  Stable periods: 137 (53.3%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: PREPARE FEATURES FOR ML\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Preparing Features for Machine Learning\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Select features for model\n",
        "feature_columns = [\n",
        "    'import_concentration',\n",
        "    'hhi',\n",
        "    'supplier_market_dominance',\n",
        "    'political_stability',\n",
        "    'regulatory_quality',\n",
        "    'control_of_corruption',\n",
        "    'fta_active',\n",
        "    'mou_minerals',\n",
        "    'price_volatility',\n",
        "    'import_value_usd',\n",
        "    'import_tonnage','price_change_pct',\n",
        "    'governance_risk_score',\n",
        "    'risk_weighted_volume'\n",
        "]\n",
        "\n",
        "print(f\"\\nSelected {len(feature_columns)} features:\")\n",
        "for i, feature in enumerate(feature_columns, 1):\n",
        "    print(f\"  {i}. {feature}\")\n",
        "\n",
        "# Create feature matrix and target\n",
        "X = combined_df[feature_columns].copy()\n",
        "y = combined_df['supply_fragility_event'].copy()\n",
        "\n",
        "print(f\"\\nDataset shape:\")\n",
        "print(f\"  Features (X): {X.shape}\")\n",
        "print(f\"  Target (y): {y.shape}\")\n",
        "\n",
        "# Check for any remaining NaN values\n",
        "nan_count = X.isnull().sum().sum()\n",
        "if nan_count > 0:\n",
        "    print(f\"\\n‚ö† WARNING: {nan_count} NaN values found in features\")\n",
        "    print(\"Filling with column medians...\")\n",
        "    X = X.fillna(X.median())\n",
        "    print(\"‚úì NaN values filled\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Splitting Data (Train/Test)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: Why split data?\n",
        "- Train set: Model learns patterns from this (80%)\n",
        "- Test set: Model evaluated on this (20%)\n",
        "- Test set is \"unseen\" - measures real-world performance\n",
        "\"\"\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,  # 20% for testing\n",
        "    random_state=42,\n",
        "    stratify=y  # Maintain class balance\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {len(X_train)} observations\")\n",
        "print(f\"  Fragility events: {y_train.sum()} ({y_train.sum()/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTest set: {len(X_test)} observations\")\n",
        "print(f\"  Fragility events: {y_test.sum()} ({y_test.sum()/len(y_test)*100:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDPslDMfn17f",
        "outputId": "051560a6-81a7-4fa3-a42b-5ef3248e23c0"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: Preparing Features for Machine Learning\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Selected 14 features:\n",
            "  1. import_concentration\n",
            "  2. hhi\n",
            "  3. supplier_market_dominance\n",
            "  4. political_stability\n",
            "  5. regulatory_quality\n",
            "  6. control_of_corruption\n",
            "  7. fta_active\n",
            "  8. mou_minerals\n",
            "  9. price_volatility\n",
            "  10. import_value_usd\n",
            "  11. import_tonnage\n",
            "  12. price_change_pct\n",
            "  13. governance_risk_score\n",
            "  14. risk_weighted_volume\n",
            "\n",
            "Dataset shape:\n",
            "  Features (X): (257, 14)\n",
            "  Target (y): (257,)\n",
            "\n",
            "================================================================================\n",
            "STEP 7: Splitting Data (Train/Test)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Train set: 205 observations\n",
            "  Fragility events: 96 (46.8%)\n",
            "\n",
            "Test set: 52 observations\n",
            "  Fragility events: 24 (46.2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 8: TRAIN RANDOM FOREST MODEL (WITH CLASS WEIGHTS)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: Training Random Forest Model (Class Weighted)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nCalculating optimal class weights...\")\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = y_train.value_counts()\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(f\"  Stable (0): {class_counts[0]} observations\")\n",
        "print(f\"  Fragility (1): {class_counts[1]} observations\")\n",
        "print(f\"  Imbalance ratio: {class_counts[0]/class_counts[1]:.2f}:1\")\n",
        "\n",
        "# Calculate custom weights (penalize false negatives more)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "base_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "\n",
        "# Boost fragility class weight by 50% (catches more disruptions)\n",
        "custom_weights = {\n",
        "    0: base_weights[0],\n",
        "    1: base_weights[1] * 1.5  # 1.5x boost for fragility events\n",
        "}\n",
        "\n",
        "print(f\"\\nClass weights:\")\n",
        "print(f\"  Stable (0): {custom_weights[0]:.3f}\")\n",
        "print(f\"  Fragility (1): {custom_weights[1]:.3f} (1.5x boosted)\")\n",
        "print(f\"  ‚Üí Model will penalize missing disruptions 1.5x more than false alarms\")\n",
        "\n",
        "print(\"\\nTraining Random Forest with weighted classes...\")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight=custom_weights,  # ‚Üê ADDED CLASS WEIGHTS\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úì Random Forest trained successfully with class weighting!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQzoXLk5n6bf",
        "outputId": "f1703f30-a89f-4d4a-c33f-5af1164ad4d3"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: Training Random Forest Model (Class Weighted)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Calculating optimal class weights...\n",
            "\n",
            "Training set class distribution:\n",
            "  Stable (0): 109 observations\n",
            "  Fragility (1): 96 observations\n",
            "  Imbalance ratio: 1.14:1\n",
            "\n",
            "Class weights:\n",
            "  Stable (0): 0.940\n",
            "  Fragility (1): 1.602 (1.5x boosted)\n",
            "  ‚Üí Model will penalize missing disruptions 1.5x more than false alarms\n",
            "\n",
            "Training Random Forest with weighted classes...\n",
            "‚úì Random Forest trained successfully with class weighting!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 9: TRAIN XGBOOST MODEL\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 9: Training XGBoost Model\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: What is XGBoost?\n",
        "- Gradient boosting algorithm\n",
        "- Builds trees sequentially (each corrects previous errors)\n",
        "- Often wins ML competitions\n",
        "- Good for: Complex patterns, high accuracy\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nTraining XGBoost (this may take 1-2 minutes)...\")\n",
        "\n",
        "# Calculate scale_pos_weight for class imbalance\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    reg_lambda=1.0,  # L2 regularization\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úì XGBoost trained successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myOhxpBaoDni",
        "outputId": "de254f42-0955-49ff-fdfd-4e41e432e68d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 9: Training XGBoost Model\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Training XGBoost (this may take 1-2 minutes)...\n",
            "‚úì XGBoost trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 10: MODEL EVALUATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 10: Evaluating Model Performance\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Cross-validation on training set\n",
        "print(\"\\n[CROSS-VALIDATION] 5-Fold on Training Set\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
        "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
        "\n",
        "print(f\"\\nRandom Forest CV ROC-AUC: {rf_cv_scores.mean():.3f} (+/- {rf_cv_scores.std():.3f})\")\n",
        "print(f\"XGBoost CV ROC-AUC: {xgb_cv_scores.mean():.3f} (+/- {xgb_cv_scores.std():.3f})\")\n",
        "\n",
        "# Test set evaluation\n",
        "print(\"\\n[TEST SET PERFORMANCE] Final Evaluation\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Random Forest predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "print(\"\\nRandom Forest:\")\n",
        "print(f\"  Test ROC-AUC: {roc_auc_rf:.3f}\")\n",
        "print(\"\\n\" + classification_report(y_test, y_pred_rf, target_names=['Stable', 'Fragility Event']))\n",
        "\n",
        "# XGBoost predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "print(\"\\nXGBoost:\")\n",
        "print(f\"  Test ROC-AUC: {roc_auc_xgb:.3f}\")\n",
        "print(\"\\n\" + classification_report(y_test, y_pred_xgb, target_names=['Stable', 'Fragility Event']))\n",
        "\n",
        "# Select best model\n",
        "if roc_auc_xgb > roc_auc_rf:\n",
        "    best_model = xgb_model\n",
        "    best_model_name = \"XGBoost\"\n",
        "    best_roc_auc = roc_auc_xgb\n",
        "    y_pred_best = y_pred_xgb\n",
        "    y_pred_proba_best = y_pred_proba_xgb\n",
        "else:\n",
        "    best_model = rf_model\n",
        "    best_model_name = \"Random Forest\"\n",
        "    best_roc_auc = roc_auc_rf\n",
        "    y_pred_best = y_pred_rf\n",
        "    y_pred_proba_best = y_pred_proba_rf\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name} (ROC-AUC: {best_roc_auc:.3f})\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (Best Model):\")\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "print(cm)\n",
        "print(f\"\\nTrue Negatives: {cm[0,0]} | False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]} | True Positives: {cm[1,1]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HHoRVuZoIF5",
        "outputId": "5d0d598f-dffa-4c1f-ceee-593ee255398f"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 10: Evaluating Model Performance\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[CROSS-VALIDATION] 5-Fold on Training Set\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Random Forest CV ROC-AUC: 0.662 (+/- 0.108)\n",
            "XGBoost CV ROC-AUC: 0.671 (+/- 0.086)\n",
            "\n",
            "[TEST SET PERFORMANCE] Final Evaluation\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Random Forest:\n",
            "  Test ROC-AUC: 0.705\n",
            "\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Stable       0.63      0.61      0.62        28\n",
            "Fragility Event       0.56      0.58      0.57        24\n",
            "\n",
            "       accuracy                           0.60        52\n",
            "      macro avg       0.59      0.60      0.59        52\n",
            "   weighted avg       0.60      0.60      0.60        52\n",
            "\n",
            "\n",
            "XGBoost:\n",
            "  Test ROC-AUC: 0.661\n",
            "\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Stable       0.62      0.71      0.67        28\n",
            "Fragility Event       0.60      0.50      0.55        24\n",
            "\n",
            "       accuracy                           0.62        52\n",
            "      macro avg       0.61      0.61      0.61        52\n",
            "   weighted avg       0.61      0.62      0.61        52\n",
            "\n",
            "\n",
            "üèÜ BEST MODEL: Random Forest (ROC-AUC: 0.705)\n",
            "\n",
            "Confusion Matrix (Best Model):\n",
            "[[17 11]\n",
            " [10 14]]\n",
            "\n",
            "True Negatives: 17 | False Positives: 11\n",
            "False Negatives: 10 | True Positives: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# THRESHOLD OPTIMIZATION TO REDUCE FALSE NEGATIVES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"THRESHOLD OPTIMIZATION: Reducing False Negatives\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "CURRENT PROBLEM: Missing 6 out of 13 disruptions (46% miss rate)\n",
        "SOLUTION: Lower the classification threshold from 0.50\n",
        "TRADE-OFF: More false alarms, but catch more real disruptions\n",
        "\"\"\"\n",
        "\n",
        "# Get probability predictions from Random Forest\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Test different thresholds\n",
        "thresholds = np.arange(0.15, 0.75, 0.05)\n",
        "results = []\n",
        "\n",
        "print(\"\\nTesting thresholds from 0.15 to 0.70...\")\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba_rf >= threshold).astype(int)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
        "\n",
        "    # Calculate metrics\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    results.append({\n",
        "        'threshold': threshold,\n",
        "        'recall': recall,\n",
        "        'precision': precision,\n",
        "        'f1': f1,\n",
        "        'false_negatives': int(fn),\n",
        "        'false_positives': int(fp),\n",
        "        'true_positives': int(tp),\n",
        "        'true_negatives': int(tn)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Find key thresholds\n",
        "max_recall_row = results_df.loc[results_df['recall'].idxmax()]\n",
        "max_f1_row = results_df.loc[results_df['f1'].idxmax()]\n",
        "target_80_rows = results_df[results_df['recall'] >= 0.80]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä THRESHOLD RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Recommendation 1: Maximize Recall\n",
        "print(f\"\\n1Ô∏è‚É£  MAXIMIZE RECALL (Catch Most Disruptions)\")\n",
        "print(f\"   Threshold: {max_recall_row['threshold']:.2f}\")\n",
        "print(f\"   Recall: {max_recall_row['recall']:.0%} (catch {int(max_recall_row['true_positives'])} of 13)\")\n",
        "print(f\"   False Negatives: {max_recall_row['false_negatives']} (down from 6) ‚úÖ\")\n",
        "print(f\"   False Positives: {max_recall_row['false_positives']} (up from 4)\")\n",
        "print(f\"   üí° Best for: Risk-averse policy (better safe than sorry)\")\n",
        "\n",
        "# Recommendation 2: Balanced\n",
        "print(f\"\\n2Ô∏è‚É£  BALANCED APPROACH (Max F1-Score)\")\n",
        "print(f\"   Threshold: {max_f1_row['threshold']:.2f}\")\n",
        "print(f\"   F1-Score: {max_f1_row['f1']:.3f}\")\n",
        "print(f\"   Recall: {max_f1_row['recall']:.0%} (catch {int(max_f1_row['true_positives'])} of 13)\")\n",
        "print(f\"   False Negatives: {max_f1_row['false_negatives']} (down from 6) ‚úÖ\")\n",
        "print(f\"   False Positives: {max_f1_row['false_positives']}\")\n",
        "print(f\"   üí° Best for: Academic standard / balanced policy\")\n",
        "\n",
        "# Recommendation 3: Target 80%\n",
        "if len(target_80_rows) > 0:\n",
        "    target_row = target_80_rows.iloc[0]\n",
        "    print(f\"\\n3Ô∏è‚É£  TARGET 80% RECALL (Policy Standard)\")\n",
        "    print(f\"   Threshold: {target_row['threshold']:.2f}\")\n",
        "    print(f\"   Recall: {target_row['recall']:.0%} (catch {int(target_row['true_positives'])} of 13)\")\n",
        "    print(f\"   False Negatives: {target_row['false_negatives']} ‚úÖ\")\n",
        "    print(f\"   False Positives: {target_row['false_positives']}\")\n",
        "    print(f\"   üí° Best for: Policy recommendation (catches 4 of 5 disruptions)\")\n",
        "    recommended_threshold = target_row['threshold']\n",
        "    recommended_label = \"80% Recall Target\"\n",
        "else:\n",
        "    print(f\"\\n3Ô∏è‚É£  TARGET 80% RECALL\")\n",
        "    print(f\"   ‚ö†Ô∏è  Not achievable with current model\")\n",
        "    print(f\"   Maximum recall: {max_recall_row['recall']:.0%}\")\n",
        "    recommended_threshold = max_f1_row['threshold']\n",
        "    recommended_label = \"Balanced (Max F1)\"\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"üéØ RECOMMENDED THRESHOLD: {recommended_threshold:.2f}\")\n",
        "print(f\"   Strategy: {recommended_label}\")\n",
        "print(f\"   Expected improvement: FN 6 ‚Üí {results_df.loc[results_df['threshold']==recommended_threshold, 'false_negatives'].values[0]}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 11))\n",
        "\n",
        "# Plot 1: Recall vs Threshold\n",
        "axes[0, 0].plot(results_df['threshold'], results_df['recall'], 'b-', linewidth=3, marker='o', markersize=4)\n",
        "axes[0, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2.5, label='Default (0.50)', alpha=0.7)\n",
        "axes[0, 0].axvline(x=recommended_threshold, color='green', linestyle='--', linewidth=2.5,\n",
        "                   label=f'Recommended ({recommended_threshold:.2f})', alpha=0.9)\n",
        "axes[0, 0].axhline(y=0.54, color='orange', linestyle=':', linewidth=2, label='Current (54%)', alpha=0.7)\n",
        "axes[0, 0].fill_between(results_df['threshold'], 0, results_df['recall'], alpha=0.2)\n",
        "axes[0, 0].set_xlabel('Threshold', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Recall (% Disruptions Caught)', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].set_title('Recall vs Threshold', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=11, loc='best')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_ylim([0, 1.05])\n",
        "\n",
        "# Plot 2: False Negatives\n",
        "axes[0, 1].plot(results_df['threshold'], results_df['false_negatives'], 'r-', linewidth=3, marker='s', markersize=4)\n",
        "axes[0, 1].axvline(x=0.5, color='red', linestyle='--', linewidth=2.5, alpha=0.7)\n",
        "axes[0, 1].axvline(x=recommended_threshold, color='green', linestyle='--', linewidth=2.5, alpha=0.9)\n",
        "axes[0, 1].axhline(y=6, color='orange', linestyle=':', linewidth=2, label='Current (6 FN)', alpha=0.7)\n",
        "axes[0, 1].fill_between(results_df['threshold'], 0, results_df['false_negatives'], color='red', alpha=0.2)\n",
        "axes[0, 1].set_xlabel('Threshold', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('False Negatives (Missed Disruptions)', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].set_title('False Negatives vs Threshold', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=11, loc='best')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Precision-Recall-F1 Trade-off\n",
        "axes[1, 0].plot(results_df['threshold'], results_df['precision'], 'g-', linewidth=2.5,\n",
        "                marker='^', markersize=4, label='Precision')\n",
        "axes[1, 0].plot(results_df['threshold'], results_df['recall'], 'b-', linewidth=2.5,\n",
        "                marker='o', markersize=4, label='Recall')\n",
        "axes[1, 0].plot(results_df['threshold'], results_df['f1'], 'purple', linewidth=2.5,\n",
        "                marker='D', markersize=4, label='F1-Score')\n",
        "axes[1, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
        "axes[1, 0].axvline(x=recommended_threshold, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
        "axes[1, 0].set_xlabel('Threshold', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Score', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].set_title('Precision-Recall-F1 Trade-off', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=11, loc='best')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_ylim([0, 1.05])\n",
        "\n",
        "# Plot 4: Confusion Matrix at Recommended Threshold\n",
        "y_pred_optimal = (y_pred_proba_rf >= recommended_threshold).astype(int)\n",
        "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
        "\n",
        "sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='RdYlGn', cbar=False,\n",
        "            xticklabels=['Stable', 'Disruption'],\n",
        "            yticklabels=['Stable', 'Disruption'],\n",
        "            ax=axes[1, 1],\n",
        "            annot_kws={'fontsize': 20, 'fontweight': 'bold'},\n",
        "            linewidths=2, linecolor='black')\n",
        "axes[1, 1].set_title(f'Optimized Confusion Matrix\\n(Threshold={recommended_threshold:.2f})',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Actual', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Add comparison text\n",
        "tn_opt, fp_opt, fn_opt, tp_opt = cm_optimal.ravel()\n",
        "improvement_text = f'Before: FN=6, FP=4 | After: FN={fn_opt}, FP={fp_opt}'\n",
        "fn_reduction = ((6 - fn_opt) / 6 * 100) if fn_opt < 6 else 0\n",
        "improvement_text += f'\\nüéØ FN Reduction: {fn_reduction:.0f}%'\n",
        "\n",
        "axes[1, 1].text(0.5, -0.18, improvement_text,\n",
        "                transform=axes[1, 1].transAxes, ha='center', fontsize=12, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8, edgecolor='black', linewidth=2))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('threshold_optimization_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n‚úì Saved: threshold_optimization_analysis.png\")\n",
        "\n",
        "# Show detailed table\n",
        "print(\"\\nüìã DETAILED THRESHOLD ANALYSIS TABLE\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Export\n",
        "results_df.to_csv('threshold_analysis.csv', index=False)\n",
        "print(\"\\n‚úì Saved: threshold_analysis.csv\")\n",
        "\n",
        "# Store for later use\n",
        "optimal_threshold = recommended_threshold\n",
        "\n",
        "print(f\"\\n‚úÖ THRESHOLD OPTIMIZATION COMPLETE\")\n",
        "print(f\"   Optimal threshold stored: {optimal_threshold:.2f}\")\n",
        "print(f\"   Will be applied to scenario predictions...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTr_4uktu7Kz",
        "outputId": "edc21aab-e9f9-4b40-a0cf-d1f2d4a21082"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "THRESHOLD OPTIMIZATION: Reducing False Negatives\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Testing thresholds from 0.15 to 0.70...\n",
            "\n",
            "================================================================================\n",
            "üìä THRESHOLD RECOMMENDATIONS\n",
            "================================================================================\n",
            "\n",
            "1Ô∏è‚É£  MAXIMIZE RECALL (Catch Most Disruptions)\n",
            "   Threshold: 0.15\n",
            "   Recall: 100% (catch 24 of 13)\n",
            "   False Negatives: 0.0 (down from 6) ‚úÖ\n",
            "   False Positives: 27.0 (up from 4)\n",
            "   üí° Best for: Risk-averse policy (better safe than sorry)\n",
            "\n",
            "2Ô∏è‚É£  BALANCED APPROACH (Max F1-Score)\n",
            "   Threshold: 0.35\n",
            "   F1-Score: 0.687\n",
            "   Recall: 96% (catch 23 of 13)\n",
            "   False Negatives: 1.0 (down from 6) ‚úÖ\n",
            "   False Positives: 20.0\n",
            "   üí° Best for: Academic standard / balanced policy\n",
            "\n",
            "3Ô∏è‚É£  TARGET 80% RECALL (Policy Standard)\n",
            "   Threshold: 0.15\n",
            "   Recall: 100% (catch 24 of 13)\n",
            "   False Negatives: 0.0 ‚úÖ\n",
            "   False Positives: 27.0\n",
            "   üí° Best for: Policy recommendation (catches 4 of 5 disruptions)\n",
            "\n",
            "================================================================================\n",
            "üéØ RECOMMENDED THRESHOLD: 0.15\n",
            "   Strategy: 80% Recall Target\n",
            "   Expected improvement: FN 6 ‚Üí 0\n",
            "================================================================================\n",
            "\n",
            "‚úì Saved: threshold_optimization_analysis.png\n",
            "\n",
            "üìã DETAILED THRESHOLD ANALYSIS TABLE\n",
            "================================================================================\n",
            " threshold   recall  precision       f1  false_negatives  false_positives  true_positives  true_negatives\n",
            "      0.15 1.000000   0.470588 0.640000                0               27              24               1\n",
            "      0.20 1.000000   0.470588 0.640000                0               27              24               1\n",
            "      0.25 1.000000   0.480000 0.648649                0               26              24               2\n",
            "      0.30 1.000000   0.489796 0.657534                0               25              24               3\n",
            "      0.35 0.958333   0.534884 0.686567                1               20              23               8\n",
            "      0.40 0.791667   0.513514 0.622951                5               18              19              10\n",
            "      0.45 0.708333   0.548387 0.618182                7               14              17              14\n",
            "      0.50 0.583333   0.560000 0.571429               10               11              14              17\n",
            "      0.55 0.541667   0.619048 0.577778               11                8              13              20\n",
            "      0.60 0.458333   0.846154 0.594595               13                2              11              26\n",
            "      0.65 0.333333   1.000000 0.500000               16                0               8              28\n",
            "      0.70 0.166667   1.000000 0.285714               20                0               4              28\n",
            "\n",
            "‚úì Saved: threshold_analysis.csv\n",
            "\n",
            "‚úÖ THRESHOLD OPTIMIZATION COMPLETE\n",
            "   Optimal threshold stored: 0.15\n",
            "   Will be applied to scenario predictions...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 10: MODEL EVALUATION\n",
        "# ============================================================================\n",
        "# ... (your existing evaluation code)\n",
        "# ... (ends with confusion matrix display)\n",
        "\n",
        "# ============================================================================\n",
        "# NEW STEP: THRESHOLD OPTIMIZATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"THRESHOLD OPTIMIZATION: Maximizing Disruption Detection\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get probability predictions from BEST model\n",
        "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Test different thresholds\n",
        "thresholds = np.arange(0.15, 0.75, 0.05)\n",
        "results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba_best >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
        "\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    results.append({\n",
        "        'threshold': threshold,\n",
        "        'recall': recall,\n",
        "        'precision': precision,\n",
        "        'f1': f1,\n",
        "        'false_negatives': fn,\n",
        "        'false_positives': fp,\n",
        "        'true_positives': tp,\n",
        "        'true_negatives': tn\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Find optimal thresholds\n",
        "max_recall_row = results_df.loc[results_df['recall'].idxmax()]\n",
        "max_f1_row = results_df.loc[results_df['f1'].idxmax()]\n",
        "\n",
        "# Target: 80% recall\n",
        "target_80_rows = results_df[results_df['recall'] >= 0.80]\n",
        "has_80_target = len(target_80_rows) > 0\n",
        "\n",
        "print(\"\\nüìä THRESHOLD RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n1. MAXIMIZE RECALL (Catch Most Disruptions)\")\n",
        "print(f\"   Threshold: {max_recall_row['threshold']:.2f}\")\n",
        "print(f\"   Recall: {max_recall_row['recall']:.1%}\")\n",
        "print(f\"   False Negatives: {max_recall_row['false_negatives']:.0f} (currently: 6)\")\n",
        "print(f\"   False Positives: {max_recall_row['false_positives']:.0f} (currently: 3)\")\n",
        "\n",
        "print(f\"\\n2. BALANCE APPROACH (Max F1-Score)\")\n",
        "print(f\"   Threshold: {max_f1_row['threshold']:.2f}\")\n",
        "print(f\"   F1-Score: {max_f1_row['f1']:.3f}\")\n",
        "print(f\"   Recall: {max_f1_row['recall']:.1%}\")\n",
        "print(f\"   False Negatives: {max_f1_row['false_negatives']:.0f}\")\n",
        "\n",
        "if has_80_target:\n",
        "    target_row = target_80_rows.iloc[0]\n",
        "    print(f\"\\n3. TARGET 80% RECALL\")\n",
        "    print(f\"   Threshold: {target_row['threshold']:.2f}\")\n",
        "    print(f\"   Recall: {target_row['recall']:.1%}\")\n",
        "    print(f\"   False Negatives: {target_row['false_negatives']:.0f}\")\n",
        "    recommended_threshold = target_row['threshold']\n",
        "else:\n",
        "    print(f\"\\n3. TARGET 80% RECALL: Not achievable\")\n",
        "    print(f\"   Maximum: {max_recall_row['recall']:.1%}\")\n",
        "    recommended_threshold = max_f1_row['threshold']\n",
        "\n",
        "print(f\"\\nüéØ RECOMMENDED THRESHOLD: {recommended_threshold:.2f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Recall vs Threshold\n",
        "axes[0, 0].plot(results_df['threshold'], results_df['recall'], 'b-', linewidth=2.5)\n",
        "axes[0, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Default (0.50)')\n",
        "axes[0, 0].axvline(x=recommended_threshold, color='green', linestyle='--', linewidth=2,\n",
        "                   label=f'Recommended ({recommended_threshold:.2f})')\n",
        "axes[0, 0].axhline(y=0.54, color='orange', linestyle=':', alpha=0.7, label='Current (54%)')\n",
        "axes[0, 0].set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title('Recall vs Threshold', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: False Negatives\n",
        "axes[0, 1].plot(results_df['threshold'], results_df['false_negatives'], 'r-', linewidth=2.5)\n",
        "axes[0, 1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
        "axes[0, 1].axvline(x=recommended_threshold, color='green', linestyle='--', linewidth=2)\n",
        "axes[0, 1].axhline(y=6, color='orange', linestyle=':', linewidth=2, label='Current (6)')\n",
        "axes[0, 1].set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('False Negatives', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('False Negatives vs Threshold', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Trade-offs\n",
        "axes[1, 0].plot(results_df['threshold'], results_df['precision'], 'g-', linewidth=2, label='Precision')\n",
        "axes[1, 0].plot(results_df['threshold'], results_df['recall'], 'b-', linewidth=2, label='Recall')\n",
        "axes[1, 0].plot(results_df['threshold'], results_df['f1'], 'purple', linewidth=2, label='F1')\n",
        "axes[1, 0].axvline(x=recommended_threshold, color='green', linestyle='--', linewidth=1.5)\n",
        "axes[1, 0].set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('Metric Trade-offs', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Optimal Confusion Matrix\n",
        "y_pred_optimal = (y_pred_proba_best >= recommended_threshold).astype(int)\n",
        "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
        "\n",
        "sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='RdYlGn', cbar=False,\n",
        "            xticklabels=['Stable', 'Disruption'],\n",
        "            yticklabels=['Stable', 'Disruption'],\n",
        "            ax=axes[1, 1],\n",
        "            annot_kws={'fontsize': 16, 'fontweight': 'bold'})\n",
        "axes[1, 1].set_title(f'Optimized (Threshold={recommended_threshold:.2f})',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Actual', fontsize=12)\n",
        "axes[1, 1].set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "tn_opt, fp_opt, fn_opt, tp_opt = cm_optimal.ravel()\n",
        "axes[1, 1].text(0.5, -0.15, f'FN: {fn_opt} (was 6) | FP: {fp_opt} (was 3)',\n",
        "                transform=axes[1, 1].transAxes, ha='center', fontsize=11, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('threshold_optimization_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n‚úì Saved: threshold_optimization_analysis.png\")\n",
        "\n",
        "# Export table\n",
        "results_df.to_csv('threshold_analysis.csv', index=False)\n",
        "print(\"‚úì Saved: threshold_analysis.csv\")\n",
        "\n",
        "# Store recommended threshold for later use\n",
        "optimal_threshold = recommended_threshold\n",
        "\n",
        "print(\"\\n‚úì Threshold optimization complete!\")\n",
        "print(f\"  Optimal threshold: {optimal_threshold:.2f}\")\n",
        "print(f\"  Expected FN reduction: {6 - fn_opt:.0f} ({(6-fn_opt)/6*100:.0f}%)\")\n",
        "\n",
        "# Continue to Step 11..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXzWbNJKqbyF",
        "outputId": "6a535b92-ab11-45d5-d875-d0b1736e5d9c"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "THRESHOLD OPTIMIZATION: Maximizing Disruption Detection\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìä THRESHOLD RECOMMENDATIONS\n",
            "================================================================================\n",
            "\n",
            "1. MAXIMIZE RECALL (Catch Most Disruptions)\n",
            "   Threshold: 0.15\n",
            "   Recall: 100.0%\n",
            "   False Negatives: 0 (currently: 6)\n",
            "   False Positives: 27 (currently: 3)\n",
            "\n",
            "2. BALANCE APPROACH (Max F1-Score)\n",
            "   Threshold: 0.35\n",
            "   F1-Score: 0.687\n",
            "   Recall: 95.8%\n",
            "   False Negatives: 1\n",
            "\n",
            "3. TARGET 80% RECALL\n",
            "   Threshold: 0.15\n",
            "   Recall: 100.0%\n",
            "   False Negatives: 0\n",
            "\n",
            "üéØ RECOMMENDED THRESHOLD: 0.15\n",
            "================================================================================\n",
            "\n",
            "‚úì Saved: threshold_optimization_analysis.png\n",
            "‚úì Saved: threshold_analysis.csv\n",
            "\n",
            "‚úì Threshold optimization complete!\n",
            "  Optimal threshold: 0.15\n",
            "  Expected FN reduction: 6 (100%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 11: FEATURE IMPORTANCE (SHAP)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 11: Analyzing Feature Importance (SHAP)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: What is SHAP?\n",
        "- Shows which features drive predictions\n",
        "- Positive SHAP = increases fragility risk\n",
        "- Negative SHAP = decreases fragility risk\n",
        "- Helps understand \"why\" model makes predictions\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nCalculating SHAP values (may take 2-3 minutes)...\")\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(best_model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Handle different SHAP output formats\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values_positive = shap_values[1]  # Positive class\n",
        "elif len(shap_values.shape) == 3:\n",
        "    shap_values_positive = shap_values[:, :, 1]  # 3D array format\n",
        "else:\n",
        "    shap_values_positive = shap_values  # 2D array format\n",
        "\n",
        "print(\"‚úì SHAP values calculated!\")\n",
        "\n",
        "# Feature importance ranking\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_columns,\n",
        "    'importance': np.abs(shap_values_positive).mean(axis=0)\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyUMMsrvoQrQ",
        "outputId": "261efd15-29b3-4e2e-f59f-5a2cecf27547"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 11: Analyzing Feature Importance (SHAP)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Calculating SHAP values (may take 2-3 minutes)...\n",
            "‚úì SHAP values calculated!\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "              feature  importance\n",
            "     price_volatility    0.059802\n",
            "     import_value_usd    0.036200\n",
            " import_concentration    0.033584\n",
            "     price_change_pct    0.033454\n",
            "  political_stability    0.033313\n",
            "                  hhi    0.030025\n",
            "       import_tonnage    0.019435\n",
            " risk_weighted_volume    0.018401\n",
            "governance_risk_score    0.015586\n",
            "   regulatory_quality    0.014896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 12: SCENARIO PROJECTIONS (2026-2030)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 12: Generating Future Scenario Projections\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: What are scenarios?\n",
        "- Conservative: Minimal policy changes, slow growth\n",
        "- Moderate: Some FTAs signed, steady growth\n",
        "- Aggressive: Many FTAs, rapid demand growth\n",
        "\"\"\"\n",
        "# At the start of Step 12\n",
        "optimal_threshold = 0.30  # Optimized to catch more disruptions\n",
        "\n",
        "# Get latest year data as baseline\n",
        "latest_year = combined_df['year'].max()\n",
        "baseline_data = combined_df[combined_df['year'] == latest_year].copy()\n",
        "\n",
        "print(f\"\\nBaseline year: {latest_year}\")\n",
        "print(f\"Baseline observations: {len(baseline_data)}\")\n",
        "\n",
        "# Define scenario parameters\n",
        "scenarios = {\n",
        "    'conservative': {\n",
        "        'import_growth': 1.03,  # 3% annual growth\n",
        "        'governance_change': 0.0,  # No improvement\n",
        "        'fta_prob': 0.0,  # No new FTAs\n",
        "        'mou_prob': 0.0   # No new MoUs\n",
        "    },\n",
        "    'moderate': {\n",
        "        'import_growth': 1.10,  # 10% annual growth\n",
        "        'governance_change': 0.05,  # Slight improvement\n",
        "        'fta_prob': 0.15,  # 15% chance new FTA\n",
        "        'mou_prob': 0.20   # 20% chance new MoU\n",
        "    },\n",
        "    'aggressive': {\n",
        "        'import_growth': 1.18,  # 18% annual growth\n",
        "        'governance_change': 0.12,  # Significant improvement\n",
        "        'fta_prob': 0.35,  # 35% chance new FTA\n",
        "        'mou_prob': 0.40   # 40% chance new MoU\n",
        "    }\n",
        "}\n",
        "\n",
        "scenario_predictions = {}\n",
        "\n",
        "for scenario_name, params in scenarios.items():\n",
        "    print(f\"\\n[{scenario_name.upper()} SCENARIO]\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    scenario_data = []\n",
        "\n",
        "    for year in range(2026, 2031):\n",
        "        year_offset = year - latest_year\n",
        "\n",
        "        # Project each supplier-mineral combination\n",
        "        for idx, row in baseline_data.iterrows():\n",
        "            projected_row = row.copy()\n",
        "            projected_row['year'] = year\n",
        "\n",
        "            # Project import volumes\n",
        "            projected_row['import_tonnage'] *= (params['import_growth'] ** year_offset)\n",
        "            projected_row['import_value_usd'] *= (params['import_growth'] ** year_offset)\n",
        "\n",
        "            # Project governance improvements\n",
        "            projected_row['political_stability'] = min(2.5, projected_row['political_stability'] + params['governance_change'] * year_offset)\n",
        "            projected_row['regulatory_quality'] = min(2.5, projected_row['regulatory_quality'] + params['governance_change'] * year_offset)\n",
        "            projected_row['control_of_corruption'] = min(2.5, projected_row['control_of_corruption'] + params['governance_change'] * year_offset)\n",
        "\n",
        "            # Project policy activations (probabilistic)\n",
        "            if projected_row['fta_active'] == 0:\n",
        "                if np.random.random() < params['fta_prob']:\n",
        "                    projected_row['fta_active'] = 1\n",
        "\n",
        "            if projected_row['mou_minerals'] == 0:\n",
        "                if np.random.random() < params['mou_prob']:\n",
        "                    projected_row['mou_minerals'] = 1\n",
        "\n",
        "            scenario_data.append(projected_row)\n",
        "\n",
        "    scenario_df = pd.DataFrame(scenario_data)\n",
        "\n",
        "    # Recalculate concentration metrics for projected years\n",
        "    scenario_df['total_annual_imports'] = scenario_df.groupby(['year', 'mineral'])['import_tonnage'].transform('sum')\n",
        "    scenario_df['import_concentration'] = scenario_df['import_tonnage'] / scenario_df['total_annual_imports']\n",
        "\n",
        "    hhi_projected = scenario_df.groupby(['year', 'mineral']).apply(\n",
        "        lambda x: (x['import_concentration'] ** 2).sum()\n",
        "    ).reset_index(name='hhi')\n",
        "\n",
        "    scenario_df = scenario_df.drop(columns=['hhi']).merge(hhi_projected, on=['year', 'mineral'], how='left')\n",
        "    scenario_df['supplier_market_dominance'] = (scenario_df['import_concentration'] > 0.5).astype(int)\n",
        "\n",
        "    # Predict fragility probability\n",
        "    X_scenario = scenario_df[feature_columns]\n",
        "    scenario_df['fragility_probability'] = best_model.predict_proba(X_scenario)[:, 1]\n",
        "\n",
        "    scenario_predictions[scenario_name] = scenario_df\n",
        "\n",
        "    # Summary statistics\n",
        "    avg_risk = scenario_df['fragility_probability'].mean()\n",
        "    high_risk_count = (scenario_df['fragility_probability'] > 0.5).sum()\n",
        "\n",
        "    print(f\"  Average fragility risk: {avg_risk:.3f}\")\n",
        "    print(f\"  High-risk observations: {high_risk_count} ({high_risk_count/len(scenario_df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n‚úì All scenarios projected!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: SCENARIO PROJECTIONS (2026-2030)\n",
        "# ============================================================================\n",
        "# ... (all your existing scenario code)\n",
        "# ... (ends with scenario_predictions dictionary being created)\n",
        "\n",
        "# ADD THIS AT THE END OF STEP 12:\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"APPLYING OPTIMIZED THRESHOLD TO SCENARIOS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\nUsing optimized threshold: {optimal_threshold:.2f} (instead of 0.50)\")\n",
        "\n",
        "for scenario_name, scenario_df in scenario_predictions.items():\n",
        "    # Original probabilities already calculated\n",
        "    # Now apply optimized threshold\n",
        "    scenario_df['high_risk_flag'] = (scenario_df['fragility_probability'] >= optimal_threshold).astype(int)\n",
        "\n",
        "    # Count with different thresholds\n",
        "    high_risk_new = scenario_df['high_risk_flag'].sum()\n",
        "    high_risk_old = (scenario_df['fragility_probability'] >= 0.50).sum()\n",
        "\n",
        "    print(f\"\\n{scenario_name.title()} Scenario:\")\n",
        "    print(f\"  High-risk (old threshold=0.50): {high_risk_old}\")\n",
        "    print(f\"  High-risk (new threshold={optimal_threshold:.2f}): {high_risk_new}\")\n",
        "    print(f\"  Additional risks identified: +{high_risk_new - high_risk_old}\")\n",
        "\n",
        "print(\"\\n‚úì Scenarios updated with optimized threshold!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heq0vqEWoljM",
        "outputId": "5cc301bd-e39b-42b0-baf1-c954b2ec29e9"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 12: Generating Future Scenario Projections\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Baseline year: 2024\n",
            "Baseline observations: 13\n",
            "\n",
            "[CONSERVATIVE SCENARIO]\n",
            "----------------------------------------\n",
            "  Average fragility risk: 0.528\n",
            "  High-risk observations: 35 (53.8%)\n",
            "\n",
            "[MODERATE SCENARIO]\n",
            "----------------------------------------\n",
            "  Average fragility risk: 0.495\n",
            "  High-risk observations: 33 (50.8%)\n",
            "\n",
            "[AGGRESSIVE SCENARIO]\n",
            "----------------------------------------\n",
            "  Average fragility risk: 0.455\n",
            "  High-risk observations: 25 (38.5%)\n",
            "\n",
            "‚úì All scenarios projected!\n",
            "\n",
            "================================================================================\n",
            "APPLYING OPTIMIZED THRESHOLD TO SCENARIOS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Using optimized threshold: 0.30 (instead of 0.50)\n",
            "\n",
            "Conservative Scenario:\n",
            "  High-risk (old threshold=0.50): 35\n",
            "  High-risk (new threshold=0.30): 50\n",
            "  Additional risks identified: +15\n",
            "\n",
            "Moderate Scenario:\n",
            "  High-risk (old threshold=0.50): 33\n",
            "  High-risk (new threshold=0.30): 51\n",
            "  Additional risks identified: +18\n",
            "\n",
            "Aggressive Scenario:\n",
            "  High-risk (old threshold=0.50): 25\n",
            "  High-risk (new threshold=0.30): 50\n",
            "  Additional risks identified: +25\n",
            "\n",
            "‚úì Scenarios updated with optimized threshold!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 13: COUNTRY-LEVEL RISK ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 13: Country-Level Risk Analysis\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "\"\"\"\n",
        "BEGINNER NOTE: What is country risk analysis?\n",
        "- Identifies which supplier countries are most vulnerable\n",
        "- Ranks countries by fragility probability\n",
        "- Shows geographic concentration of risk\n",
        "- Helps prioritize diplomatic/policy interventions\n",
        "\"\"\"\n",
        "\n",
        "# ========================================================================\n",
        "# ANALYZE CURRENT SUPPLIER RISK (HISTORICAL DATA)\n",
        "# ========================================================================\n",
        "print(\"\\n[HISTORICAL RISK] Analyzing Current Supplier Vulnerabilities...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Get latest year's actual risk assessment\n",
        "latest_year = combined_df['year'].max()\n",
        "current_suppliers = combined_df[combined_df['year'] == latest_year].copy()\n",
        "\n",
        "# Calculate risk metrics per country\n",
        "country_risk_current = current_suppliers.groupby('supplier_country').agg({\n",
        "    'supply_fragility_event': 'mean',  # Historical fragility rate\n",
        "    'import_tonnage': 'sum',  # Total imports\n",
        "    'import_concentration': 'mean',  # Average concentration\n",
        "    'political_stability': 'mean',\n",
        "    'regulatory_quality': 'mean',\n",
        "    'control_of_corruption': 'mean',\n",
        "    'fta_active': 'max',\n",
        "    'mou_minerals': 'max',\n",
        "    'mineral': lambda x: ', '.join(x.unique())  # Which minerals\n",
        "}).round(3)\n",
        "\n",
        "country_risk_current.columns = [\n",
        "    'historical_fragility_rate',\n",
        "    'total_import_tonnage',\n",
        "    'avg_concentration',\n",
        "    'political_stability',\n",
        "    'regulatory_quality',\n",
        "    'control_of_corruption',\n",
        "    'has_fta',\n",
        "    'has_mou',\n",
        "    'minerals_supplied'\n",
        "]\n",
        "\n",
        "# Calculate composite risk score\n",
        "country_risk_current['composite_risk_score'] = (\n",
        "    country_risk_current['historical_fragility_rate'] * 0.4 +  # 40% weight\n",
        "    (1 - (country_risk_current['political_stability'] + 2.5) / 5.0) * 0.3 +  # 30% weight (normalized)\n",
        "    (1 - (country_risk_current['regulatory_quality'] + 2.5) / 5.0) * 0.2 +  # 20% weight\n",
        "    country_risk_current['avg_concentration'] * 0.1  # 10% weight\n",
        ").round(3)\n",
        "\n",
        "# Sort by composite risk\n",
        "country_risk_current = country_risk_current.sort_values('composite_risk_score', ascending=False)\n",
        "\n",
        "print(\"\\nüìä TOP 10 HIGHEST-RISK SUPPLIER COUNTRIES (Current)\")\n",
        "print(\"=\" * 80)\n",
        "print(country_risk_current.head(10).to_string())\n",
        "\n",
        "print(\"\\nüìä TOP 5 LOWEST-RISK SUPPLIER COUNTRIES (Current)\")\n",
        "print(\"=\" * 80)\n",
        "print(country_risk_current.tail(5).to_string())\n",
        "\n",
        "# ========================================================================\n",
        "# ANALYZE FUTURE RISK BY SCENARIO (2026-2030)\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[FUTURE RISK] Projecting Supplier Risk (2026-2030)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "scenario_country_risk = {}\n",
        "\n",
        "for scenario_name, scenario_df in scenario_predictions.items():\n",
        "    print(f\"\\n[{scenario_name.upper()} SCENARIO]\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Calculate average risk per country across 2026-2030\n",
        "    country_risk_future = scenario_df.groupby('supplier_country').agg({\n",
        "        'fragility_probability': 'mean',  # Average predicted risk\n",
        "        'import_tonnage': 'mean',  # Average projected tonnage\n",
        "        'import_concentration': 'mean',  # Average concentration\n",
        "        'political_stability': 'mean',\n",
        "        'regulatory_quality': 'mean',\n",
        "        'control_of_corruption': 'mean',\n",
        "        'fta_active': 'max',  # Any FTA in period\n",
        "        'mou_minerals': 'max',  # Any MoU in period\n",
        "        'mineral': lambda x: ', '.join(x.unique())\n",
        "    }).round(3)\n",
        "\n",
        "    country_risk_future.columns = [\n",
        "        'avg_fragility_probability',\n",
        "        'avg_import_tonnage',\n",
        "        'avg_concentration',\n",
        "        'avg_political_stability',\n",
        "        'avg_regulatory_quality',\n",
        "        'avg_control_of_corruption',\n",
        "        'has_fta',\n",
        "        'has_mou',\n",
        "        'minerals_supplied'\n",
        "    ]\n",
        "\n",
        "    # Sort by fragility probability\n",
        "    country_risk_future = country_risk_future.sort_values('avg_fragility_probability', ascending=False)\n",
        "\n",
        "    # Store for comparison\n",
        "    scenario_country_risk[scenario_name] = country_risk_future\n",
        "\n",
        "    # Display top risks\n",
        "    print(f\"\\n  Top 5 Highest-Risk Countries:\")\n",
        "    for idx, (country, row) in enumerate(country_risk_future.head(5).iterrows(), 1):\n",
        "        print(f\"    {idx}. {country:20s} - Risk: {row['avg_fragility_probability']:.3f} | \"\n",
        "              f\"Tonnage: {row['avg_import_tonnage']:>8,.0f} | \"\n",
        "              f\"PS: {row['avg_political_stability']:>5.2f} | \"\n",
        "              f\"Minerals: {row['minerals_supplied']}\")\n",
        "\n",
        "    # Count high-risk countries\n",
        "    high_risk_count = (country_risk_future['avg_fragility_probability'] > 0.5).sum()\n",
        "    print(f\"\\n  Countries with >50% fragility risk: {high_risk_count}\")\n",
        "\n",
        "# ========================================================================\n",
        "# CROSS-SCENARIO COMPARISON\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[SCENARIO COMPARISON] Risk Change by Country\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Compare conservative vs aggressive scenarios\n",
        "conservative_risk = scenario_country_risk['conservative']['avg_fragility_probability']\n",
        "aggressive_risk = scenario_country_risk['aggressive']['avg_fragility_probability']\n",
        "\n",
        "# Calculate risk reduction from policy interventions\n",
        "risk_comparison = pd.DataFrame({\n",
        "    'conservative_risk': conservative_risk,\n",
        "    'aggressive_risk': aggressive_risk\n",
        "})\n",
        "\n",
        "risk_comparison['risk_reduction'] = (\n",
        "    (risk_comparison['conservative_risk'] - risk_comparison['aggressive_risk']) /\n",
        "    risk_comparison['conservative_risk'] * 100\n",
        ").round(1)\n",
        "\n",
        "risk_comparison = risk_comparison.sort_values('risk_reduction', ascending=False)\n",
        "\n",
        "print(\"\\nüìä TOP 10 COUNTRIES: Greatest Risk Reduction from Policy Interventions\")\n",
        "print(\"(Conservative ‚Üí Aggressive Scenario)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for country, row in risk_comparison.head(10).iterrows():\n",
        "    print(f\"{country:20s} | \"\n",
        "          f\"Conservative: {row['conservative_risk']:.3f} ‚Üí \"\n",
        "          f\"Aggressive: {row['aggressive_risk']:.3f} | \"\n",
        "          f\"Reduction: {row['risk_reduction']:>5.1f}%\")\n",
        "\n",
        "# ========================================================================\n",
        "# MINERAL-COUNTRY RISK MATRIX\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[DETAILED ANALYSIS] Mineral-Country Risk Matrix (Moderate Scenario)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create matrix: rows = countries, columns = minerals, values = risk\n",
        "moderate_data = scenario_predictions['moderate']\n",
        "\n",
        "mineral_country_matrix = moderate_data.pivot_table(\n",
        "    values='fragility_probability',\n",
        "    index='supplier_country',\n",
        "    columns='mineral',\n",
        "    aggfunc='mean'\n",
        ").round(3)\n",
        "\n",
        "print(\"\\nRisk Matrix (rows = countries, columns = minerals):\")\n",
        "print(mineral_country_matrix.to_string())\n",
        "\n",
        "# Find highest risk combinations\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üö® TOP 10 HIGHEST-RISK MINERAL-COUNTRY COMBINATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Flatten the matrix and sort\n",
        "risk_combinations = []\n",
        "for country in mineral_country_matrix.index:\n",
        "    for mineral in mineral_country_matrix.columns:\n",
        "        risk = mineral_country_matrix.loc[country, mineral]\n",
        "        if pd.notna(risk):  # Skip NaN values\n",
        "            risk_combinations.append({\n",
        "                'country': country,\n",
        "                'mineral': mineral,\n",
        "                'risk': risk\n",
        "            })\n",
        "\n",
        "risk_combinations_df = pd.DataFrame(risk_combinations).sort_values('risk', ascending=False)\n",
        "\n",
        "for idx, row in risk_combinations_df.head(10).iterrows():\n",
        "    print(f\"  {idx+1}. {row['country']:15s} - {row['mineral']:15s} | Risk: {row['risk']:.3f}\")\n",
        "\n",
        "# ========================================================================\n",
        "# CONCENTRATION ANALYSIS BY COUNTRY (ROBUST VERSION)\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[CONCENTRATION] Import Dependency by Country\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Get moderate scenario data\n",
        "moderate_data_agg = scenario_predictions['moderate'].copy()\n",
        "\n",
        "# Safety: Remove invalid rows\n",
        "moderate_data_agg = moderate_data_agg[\n",
        "    (moderate_data_agg['import_tonnage'] > 0) &\n",
        "    (moderate_data_agg['import_tonnage'].notna())\n",
        "]\n",
        "\n",
        "print(f\"Valid data rows: {len(moderate_data_agg)}\")\n",
        "\n",
        "# Calculate total imports\n",
        "total_imports = moderate_data_agg['import_tonnage'].sum()\n",
        "\n",
        "if total_imports <= 0:\n",
        "    print(\"‚ùå ERROR: Cannot calculate concentration (zero total imports)\")\n",
        "    print(\"Skipping concentration analysis...\")\n",
        "    country_import_share = pd.DataFrame(columns=['total_tonnage', 'avg_risk', 'num_minerals', 'import_share_pct']) # Initialize empty DataFrame\n",
        "else:\n",
        "    print(f\"Total imports: {total_imports:,.0f} tonnes\")\n",
        "\n",
        "    # Calculate by country\n",
        "    country_import_share = moderate_data_agg.groupby('supplier_country').agg({\n",
        "        'import_tonnage': 'sum',\n",
        "        'fragility_probability': 'mean',\n",
        "        'mineral': 'nunique'\n",
        "    })\n",
        "\n",
        "    # Calculate share percentage\n",
        "    country_import_share['import_share_pct'] = (\n",
        "        country_import_share['import_tonnage'] / total_imports * 100\n",
        "    ).clip(lower=0).round(1)  # Clip to ensure non-negative\n",
        "\n",
        "    country_import_share.columns = [\n",
        "        'total_tonnage',\n",
        "        'avg_risk',\n",
        "        'num_minerals',\n",
        "        'import_share_pct'\n",
        "    ]\n",
        "\n",
        "    country_import_share = country_import_share.sort_values('import_share_pct', ascending=False)\n",
        "\n",
        "    print(\"\\nüìä IMPORT CONCENTRATION: Top 10 Suppliers by Volume Share\")\n",
        "    print(\"=\" * 80)\n",
        "    print(country_import_share.head(10).to_string())\n",
        "\n",
        "    # Calculate cumulative concentration\n",
        "    cumulative_share = country_import_share['import_share_pct'].cumsum()\n",
        "\n",
        "    top_3_share = cumulative_share.iloc[2] if len(cumulative_share) >= 3 else cumulative_share.iloc[-1]\n",
        "    top_5_share = cumulative_share.iloc[4] if len(cumulative_share) >= 5 else cumulative_share.iloc[-1]\n",
        "\n",
        "    print(f\"\\n‚ö†Ô∏è  CONCENTRATION METRICS:\")\n",
        "    print(f\"   Top 3 countries: {top_3_share:.1f}% of total imports\")\n",
        "    print(f\"   Top 5 countries: {top_5_share:.1f}% of total imports\")\n",
        "\n",
        "    if top_3_share > 70:\n",
        "        print(f\"   üö® HIGH CONCENTRATION RISK\")\n",
        "    elif top_3_share > 50:\n",
        "        print(f\"   ‚ö†Ô∏è  MODERATE CONCENTRATION\")\n",
        "    else:\n",
        "        print(f\"   ‚úì DIVERSIFIED SUPPLY\")\n",
        "\n",
        "# ========================================================================\n",
        "# POLICY RECOMMENDATIONS BY COUNTRY\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[POLICY RECOMMENDATIONS] Country-Specific Actions\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Identify countries needing intervention\n",
        "moderate_country_risk = scenario_country_risk['moderate']\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "for country, row in moderate_country_risk.iterrows():\n",
        "    risk = row['avg_fragility_probability']\n",
        "    has_fta = row['has_fta']\n",
        "    has_mou = row['has_mou']\n",
        "    tonnage = row['avg_import_tonnage']\n",
        "\n",
        "    # High risk + high volume + no FTA = Priority 1\n",
        "    if risk > 0.6 and tonnage > 10000 and has_fta == 0:\n",
        "        recommendations.append({\n",
        "            'country': country,\n",
        "            'priority': 'URGENT',\n",
        "            'risk': risk,\n",
        "            'action': 'Negotiate FTA immediately',\n",
        "            'rationale': f'High risk ({risk:.2f}) + High volume ({tonnage:,.0f} tonnes) + No FTA'\n",
        "        })\n",
        "\n",
        "    # High risk + no MoU = Priority 2\n",
        "    elif risk > 0.5 and has_mou == 0:\n",
        "        recommendations.append({\n",
        "            'country': country,\n",
        "            'priority': 'HIGH',\n",
        "            'risk': risk,\n",
        "            'action': 'Sign Minerals MoU',\n",
        "            'rationale': f'High risk ({risk:.2f}) + No minerals agreement'\n",
        "        })\n",
        "\n",
        "    # Moderate risk + high volume = Priority 3\n",
        "    elif risk > 0.4 and tonnage > 5000:\n",
        "        recommendations.append({\n",
        "            'country': country,\n",
        "            'priority': 'MEDIUM',\n",
        "            'risk': risk,\n",
        "            'action': 'Monitor closely, consider strategic reserves',\n",
        "            'rationale': f'Moderate risk ({risk:.2f}) + Significant volume ({tonnage:,.0f} tonnes)'\n",
        "        })\n",
        "\n",
        "if recommendations: # Only create and sort if there are recommendations\n",
        "    recommendations_df = pd.DataFrame(recommendations).sort_values(['priority', 'risk'], ascending=[True, False])\n",
        "else:\n",
        "    recommendations_df = pd.DataFrame(columns=['country', 'priority', 'risk', 'action', 'rationale']) # Create empty DataFrame with expected columns\n",
        "    print(\"\\nNo policy recommendations generated based on current thresholds.\")\n",
        "\n",
        "print(\"\\nüéØ PRIORITY COUNTRY ACTIONS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if not recommendations_df.empty:\n",
        "    for idx, rec in recommendations_df.iterrows():\n",
        "        print(f\"\\n[{rec['priority']}] {rec['country']}\")\n",
        "        print(f\"  Risk Score: {rec['risk']:.3f}\")\n",
        "        print(f\"  Action: {rec['action']}\")\n",
        "        print(f\"  Rationale: {rec['rationale']}\")\n",
        "else:\n",
        "    print(\"No priority country actions to display.\")\n",
        "\n",
        "# ========================================================================\n",
        "# EXPORT COUNTRY RISK ANALYSIS\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPORTING COUNTRY RISK ANALYSIS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Export current country risk\n",
        "country_risk_current.to_csv('country_risk_current.csv')\n",
        "print(\"‚úì Saved: country_risk_current.csv\")\n",
        "\n",
        "# Export scenario-specific country risk\n",
        "for scenario_name, country_risk_df in scenario_country_risk.items():\n",
        "    filename = f'country_risk_{scenario_name}_2026_2030.csv'\n",
        "    country_risk_df.to_csv(filename)\n",
        "    print(f\"‚úì Saved: {filename}\")\n",
        "\n",
        "# Export risk comparison\n",
        "risk_comparison.to_csv('country_risk_comparison.csv')\n",
        "print(\"‚úì Saved: country_risk_comparison.csv\")\n",
        "\n",
        "# Export mineral-country matrix\n",
        "mineral_country_matrix.to_csv('mineral_country_risk_matrix.csv')\n",
        "print(\"‚úì Saved: mineral_country_risk_matrix.csv\")\n",
        "\n",
        "# Export concentration analysis\n",
        "if 'country_import_share' in locals() and not country_import_share.empty: # Check if it exists and is not empty\n",
        "    country_import_share.to_csv('country_import_concentration.csv')\n",
        "    print(\"‚úì Saved: country_import_concentration.csv\")\n",
        "else:\n",
        "    print(\"Skipping country_import_concentration.csv export as no import share data was generated.\")\n",
        "\n",
        "# Export policy recommendations\n",
        "if not recommendations_df.empty: # Only export if DataFrame is not empty\n",
        "    recommendations_df.to_csv('country_policy_recommendations.csv', index=False)\n",
        "    print(\"‚úì Saved: country_policy_recommendations.csv\")\n",
        "else:\n",
        "    print(\"Skipping country_policy_recommendations.csv export as no recommendations were generated.\")\n",
        "\n",
        "print(\"\\n‚úì Country-level risk analysis complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdH0kXC1pm7Y",
        "outputId": "36bf18ba-949d-47fe-a7c9-48e9c84ae104"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 13: Country-Level Risk Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[HISTORICAL RISK] Analyzing Current Supplier Vulnerabilities...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìä TOP 10 HIGHEST-RISK SUPPLIER COUNTRIES (Current)\n",
            "================================================================================\n",
            "                  historical_fragility_rate  total_import_tonnage  avg_concentration  political_stability  regulatory_quality  control_of_corruption  has_fta  has_mou minerals_supplied  composite_risk_score\n",
            "supplier_country                                                                                                                                                                                              \n",
            "Russia                                  1.0               236.000              0.090               -1.134              -1.122                 -1.105      0.0      0.0           lithium                 0.772\n",
            "Chile                                   1.0               907.600              0.346                0.136               0.926                  0.969      0.0      1.0           lithium                 0.639\n",
            "USA                                     1.0                86.071              0.033                0.029               1.394                  1.123      0.0      1.0           lithium                 0.596\n",
            "Belgium                                 1.0                52.250              0.022                0.404               1.168                  1.337      0.0      0.0   cobalt, lithium                 0.581\n",
            "Ireland                                 1.0               400.000              0.153                0.902               1.749                  1.575      0.0      0.0           lithium                 0.541\n",
            "China                                   0.5               902.118              0.207               -0.513              -0.357                 -0.005      0.0      0.0   cobalt, lithium                 0.516\n",
            "Germany                                 0.5               166.613              0.080                0.587               1.457                  1.664      0.0      0.0   cobalt, lithium                 0.365\n",
            "Finland                                 0.0               385.300              0.427                0.714               1.765                  2.222      0.0      0.0            cobalt                 0.179\n",
            "Canada                                  0.0               257.000              0.285                0.822               1.645                  1.672      0.0      0.0            cobalt                 0.163\n",
            "Japan                                   0.0               104.947              0.040                0.951               1.469                  1.396      1.0      1.0           lithium                 0.138\n",
            "\n",
            "üìä TOP 5 LOWEST-RISK SUPPLIER COUNTRIES (Current)\n",
            "================================================================================\n",
            "                  historical_fragility_rate  total_import_tonnage  avg_concentration  political_stability  regulatory_quality  control_of_corruption  has_fta  has_mou minerals_supplied  composite_risk_score\n",
            "supplier_country                                                                                                                                                                                              \n",
            "China                                   0.5               902.118              0.207               -0.513              -0.357                 -0.005      0.0      0.0   cobalt, lithium                 0.516\n",
            "Germany                                 0.5               166.613              0.080                0.587               1.457                  1.664      0.0      0.0   cobalt, lithium                 0.365\n",
            "Finland                                 0.0               385.300              0.427                0.714               1.765                  2.222      0.0      0.0            cobalt                 0.179\n",
            "Canada                                  0.0               257.000              0.285                0.822               1.645                  1.672      0.0      0.0            cobalt                 0.163\n",
            "Japan                                   0.0               104.947              0.040                0.951               1.469                  1.396      1.0      1.0           lithium                 0.138\n",
            "\n",
            "================================================================================\n",
            "[FUTURE RISK] Projecting Supplier Risk (2026-2030)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[CONSERVATIVE SCENARIO]\n",
            "----------------------------------------\n",
            "\n",
            "  Top 5 Highest-Risk Countries:\n",
            "    1. USA                  - Risk: 0.863 | Tonnage:       97 | PS:  0.03 | Minerals: lithium\n",
            "    2. Belgium              - Risk: 0.795 | Tonnage:       29 | PS:  0.40 | Minerals: cobalt, lithium\n",
            "    3. Chile                - Risk: 0.622 | Tonnage:    1,022 | PS:  0.14 | Minerals: lithium\n",
            "    4. Russia               - Risk: 0.615 | Tonnage:      266 | PS: -1.13 | Minerals: lithium\n",
            "    5. China                - Risk: 0.520 | Tonnage:      508 | PS: -0.51 | Minerals: cobalt, lithium\n",
            "\n",
            "  Countries with >50% fragility risk: 5\n",
            "\n",
            "[MODERATE SCENARIO]\n",
            "----------------------------------------\n",
            "\n",
            "  Top 5 Highest-Risk Countries:\n",
            "    1. USA                  - Risk: 0.839 | Tonnage:      127 | PS:  0.23 | Minerals: lithium\n",
            "    2. Belgium              - Risk: 0.741 | Tonnage:       39 | PS:  0.60 | Minerals: cobalt, lithium\n",
            "    3. Chile                - Risk: 0.560 | Tonnage:    1,341 | PS:  0.34 | Minerals: lithium\n",
            "    4. China                - Risk: 0.529 | Tonnage:      666 | PS: -0.31 | Minerals: cobalt, lithium\n",
            "    5. Russia               - Risk: 0.515 | Tonnage:      349 | PS: -0.93 | Minerals: lithium\n",
            "\n",
            "  Countries with >50% fragility risk: 5\n",
            "\n",
            "[AGGRESSIVE SCENARIO]\n",
            "----------------------------------------\n",
            "\n",
            "  Top 5 Highest-Risk Countries:\n",
            "    1. USA                  - Risk: 0.788 | Tonnage:      171 | PS:  0.51 | Minerals: lithium\n",
            "    2. Belgium              - Risk: 0.668 | Tonnage:       52 | PS:  0.88 | Minerals: cobalt, lithium\n",
            "    3. China                - Risk: 0.521 | Tonnage:      899 | PS: -0.03 | Minerals: cobalt, lithium\n",
            "    4. Chile                - Risk: 0.507 | Tonnage:    1,808 | PS:  0.62 | Minerals: lithium\n",
            "    5. Russia               - Risk: 0.493 | Tonnage:      470 | PS: -0.65 | Minerals: lithium\n",
            "\n",
            "  Countries with >50% fragility risk: 4\n",
            "\n",
            "================================================================================\n",
            "[SCENARIO COMPARISON] Risk Change by Country\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìä TOP 10 COUNTRIES: Greatest Risk Reduction from Policy Interventions\n",
            "(Conservative ‚Üí Aggressive Scenario)\n",
            "================================================================================\n",
            "Germany              | Conservative: 0.478 ‚Üí Aggressive: 0.334 | Reduction:  30.1%\n",
            "Japan                | Conservative: 0.441 ‚Üí Aggressive: 0.346 | Reduction:  21.5%\n",
            "Russia               | Conservative: 0.615 ‚Üí Aggressive: 0.493 | Reduction:  19.8%\n",
            "Chile                | Conservative: 0.622 ‚Üí Aggressive: 0.507 | Reduction:  18.5%\n",
            "Ireland              | Conservative: 0.455 ‚Üí Aggressive: 0.377 | Reduction:  17.1%\n",
            "Belgium              | Conservative: 0.795 ‚Üí Aggressive: 0.668 | Reduction:  16.0%\n",
            "USA                  | Conservative: 0.863 ‚Üí Aggressive: 0.788 | Reduction:   8.7%\n",
            "China                | Conservative: 0.520 ‚Üí Aggressive: 0.521 | Reduction:  -0.2%\n",
            "Canada               | Conservative: 0.188 ‚Üí Aggressive: 0.201 | Reduction:  -6.9%\n",
            "Finland              | Conservative: 0.095 ‚Üí Aggressive: 0.157 | Reduction: -65.3%\n",
            "\n",
            "================================================================================\n",
            "[DETAILED ANALYSIS] Mineral-Country Risk Matrix (Moderate Scenario)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Risk Matrix (rows = countries, columns = minerals):\n",
            "mineral           cobalt  lithium\n",
            "supplier_country                 \n",
            "Belgium            0.653    0.829\n",
            "Canada             0.212      NaN\n",
            "Chile                NaN    0.560\n",
            "China              0.447    0.611\n",
            "Finland            0.139      NaN\n",
            "Germany            0.292    0.496\n",
            "Ireland              NaN    0.434\n",
            "Japan                NaN    0.411\n",
            "Russia               NaN    0.515\n",
            "USA                  NaN    0.839\n",
            "\n",
            "================================================================================\n",
            "üö® TOP 10 HIGHEST-RISK MINERAL-COUNTRY COMBINATIONS\n",
            "================================================================================\n",
            "  13. USA             - lithium         | Risk: 0.839\n",
            "  2. Belgium         - lithium         | Risk: 0.829\n",
            "  1. Belgium         - cobalt          | Risk: 0.653\n",
            "  6. China           - lithium         | Risk: 0.611\n",
            "  4. Chile           - lithium         | Risk: 0.560\n",
            "  12. Russia          - lithium         | Risk: 0.515\n",
            "  9. Germany         - lithium         | Risk: 0.496\n",
            "  5. China           - cobalt          | Risk: 0.447\n",
            "  10. Ireland         - lithium         | Risk: 0.434\n",
            "  11. Japan           - lithium         | Risk: 0.411\n",
            "\n",
            "================================================================================\n",
            "[CONCENTRATION] Import Dependency by Country\n",
            "--------------------------------------------------------------------------------\n",
            "Valid data rows: 65\n",
            "Total imports: 25,840 tonnes\n",
            "\n",
            "üìä IMPORT CONCENTRATION: Top 10 Suppliers by Volume Share\n",
            "================================================================================\n",
            "                  total_tonnage  avg_risk  num_minerals  import_share_pct\n",
            "supplier_country                                                         \n",
            "Chile               6704.596400  0.560015             1              25.9\n",
            "China               6664.099928  0.529354             2              25.8\n",
            "Ireland             2954.868400  0.434314             1              11.4\n",
            "Finland             2846.276986  0.138759             1              11.0\n",
            "Canada              1898.502947  0.211546             1               7.3\n",
            "Russia              1743.372356  0.515437             1               6.7\n",
            "Germany             1230.797909  0.393784             2               4.8\n",
            "Japan                775.261435  0.410925             1               3.0\n",
            "USA                  635.821195  0.838649             1               2.5\n",
            "Belgium              385.979685  0.741348             2               1.5\n",
            "\n",
            "‚ö†Ô∏è  CONCENTRATION METRICS:\n",
            "   Top 3 countries: 63.1% of total imports\n",
            "   Top 5 countries: 81.4% of total imports\n",
            "   ‚ö†Ô∏è  MODERATE CONCENTRATION\n",
            "\n",
            "================================================================================\n",
            "[POLICY RECOMMENDATIONS] Country-Specific Actions\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "No policy recommendations generated based on current thresholds.\n",
            "\n",
            "üéØ PRIORITY COUNTRY ACTIONS:\n",
            "================================================================================\n",
            "No priority country actions to display.\n",
            "\n",
            "================================================================================\n",
            "EXPORTING COUNTRY RISK ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "‚úì Saved: country_risk_current.csv\n",
            "‚úì Saved: country_risk_conservative_2026_2030.csv\n",
            "‚úì Saved: country_risk_moderate_2026_2030.csv\n",
            "‚úì Saved: country_risk_aggressive_2026_2030.csv\n",
            "‚úì Saved: country_risk_comparison.csv\n",
            "‚úì Saved: mineral_country_risk_matrix.csv\n",
            "‚úì Saved: country_import_concentration.csv\n",
            "Skipping country_policy_recommendations.csv export as no recommendations were generated.\n",
            "\n",
            "‚úì Country-level risk analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 13: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 13: Creating Visualizations\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Figure 1: ROC Curves\n",
        "print(\"\\n[VIZ 1] Creating ROC curves...\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Random Forest ROC\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.3f})', linewidth=2)\n",
        "\n",
        "# XGBoost ROC\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
        "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_xgb:.3f})', linewidth=2)\n",
        "\n",
        "# Random baseline\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_curve_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: roc_curve_comparison.png\")\n",
        "\n",
        "# Figure 2: SHAP Feature Importance\n",
        "print(\"\\n[VIZ 2] Creating SHAP importance plot...\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values_positive, X_test, feature_names=feature_columns, show=False, plot_size=(10, 8))\n",
        "plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('shap_importance_bar.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: shap_importance_bar.png\")\n",
        "\n",
        "# Figure 3: SHAP Summary (Detailed)\n",
        "print(\"\\n[VIZ 3] Creating detailed SHAP summary...\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values_positive, X_test, feature_names=feature_columns, show=False, plot_type='dot', plot_size=(10, 8))\n",
        "plt.title('SHAP Value Distribution by Feature', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('shap_summary_detailed.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: shap_summary_detailed.png\")\n",
        "\n",
        "# Figure 4: Scenario Comparison Heatmap\n",
        "print(\"\\n[VIZ 4] Creating scenario comparison heatmap...\")\n",
        "\n",
        "# Get unique minerals and years\n",
        "minerals = scenario_predictions['moderate']['mineral'].unique()\n",
        "years = sorted(scenario_predictions['moderate']['year'].unique())\n",
        "\n",
        "# Create risk matrix for each scenario\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for idx, (scenario_name, scenario_df) in enumerate(scenario_predictions.items()):\n",
        "    # Calculate average risk per mineral per year\n",
        "    risk_matrix = scenario_df.pivot_table(\n",
        "        values='fragility_probability',\n",
        "        index='mineral',\n",
        "        columns='year',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    # Plot heatmap\n",
        "    sns.heatmap(\n",
        "        risk_matrix,\n",
        "        annot=True,\n",
        "        fmt='.2f',\n",
        "        cmap='RdYlGn_r',\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        cbar_kws={'label': 'Fragility Probability'},\n",
        "        ax=axes[idx]\n",
        "    )\n",
        "\n",
        "    axes[idx].set_title(f'{scenario_name.title()} Scenario', fontsize=14, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Year', fontsize=12)\n",
        "    axes[idx].set_ylabel('Mineral' if idx == 0 else '', fontsize=12)\n",
        "\n",
        "plt.suptitle('Supply Chain Vulnerability by Scenario (2026-2030)', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('scenario_comparison_minerals.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: scenario_comparison_minerals.png\")\n",
        "\n",
        "# Figure 5: Combined Risk Heatmap (All Scenarios)\n",
        "print(\"\\n[VIZ 5] Creating combined risk heatmap...\")\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
        "\n",
        "for idx, (scenario_name, scenario_df) in enumerate(scenario_predictions.items()):\n",
        "    risk_matrix = scenario_df.pivot_table(\n",
        "        values='fragility_probability',\n",
        "        index='mineral',\n",
        "        columns='year',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    sns.heatmap(\n",
        "        risk_matrix,\n",
        "        annot=True,\n",
        "        fmt='.2f',\n",
        "        cmap='RdYlGn_r',\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        cbar=True,\n",
        "        ax=axes[idx]\n",
        "    )\n",
        "\n",
        "    axes[idx].set_title(f'{scenario_name.title()} Scenario', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Year' if idx == 2 else '', fontsize=10)\n",
        "    axes[idx].set_ylabel('Mineral', fontsize=10)\n",
        "\n",
        "plt.suptitle('Supply Chain Vulnerability Heatmap: All Scenarios (2026-2030)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('risk_heatmap_all_scenarios.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: risk_heatmap_all_scenarios.png\")\n",
        "\n",
        "# ========================================================================\n",
        "# ADDITIONAL VISUALIZATIONS: COUNTRY RISK\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n[VIZ] Creating country risk visualizations...\")\n",
        "\n",
        "# VIZ 1: Top 10 Riskiest Countries (Bar Chart)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "moderate_country_risk = scenario_country_risk['moderate'].head(10)\n",
        "\n",
        "colors = ['red' if x > 0.6 else 'orange' if x > 0.4 else 'yellow'\n",
        "          for x in moderate_country_risk['avg_fragility_probability']]\n",
        "\n",
        "plt.barh(range(len(moderate_country_risk)),\n",
        "         moderate_country_risk['avg_fragility_probability'],\n",
        "         color=colors,\n",
        "         edgecolor='black',\n",
        "         linewidth=1.5)\n",
        "\n",
        "plt.yticks(range(len(moderate_country_risk)), moderate_country_risk.index)\n",
        "plt.xlabel('Average Fragility Probability (2026-2030)', fontsize=12)\n",
        "plt.ylabel('Supplier Country', fontsize=12)\n",
        "plt.title('Top 10 Highest-Risk Supplier Countries\\nModerate Scenario',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlim(0, 1)\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, alpha=0.5, label='High Risk Threshold')\n",
        "plt.legend()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('country_risk_ranking.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: country_risk_ranking.png\")\n",
        "\n",
        "# VIZ 2: Risk vs Import Volume (Scatter Plot)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "moderate_data_viz = scenario_country_risk['moderate']\n",
        "\n",
        "plt.scatter(moderate_data_viz['avg_import_tonnage'],\n",
        "            moderate_data_viz['avg_fragility_probability'],\n",
        "            s=200,\n",
        "            alpha=0.6,\n",
        "            c=moderate_data_viz['avg_fragility_probability'],\n",
        "            cmap='RdYlGn_r',\n",
        "            edgecolors='black',\n",
        "            linewidth=1.5)\n",
        "\n",
        "# Add country labels\n",
        "for country, row in moderate_data_viz.iterrows():\n",
        "    plt.annotate(country,\n",
        "                (row['avg_import_tonnage'], row['avg_fragility_probability']),\n",
        "                fontsize=9,\n",
        "                xytext=(5, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "plt.xlabel('Average Import Volume (tonnes/year)', fontsize=12)\n",
        "plt.ylabel('Average Fragility Probability', fontsize=12)\n",
        "plt.title('Supplier Country Risk vs Import Dependence\\nModerate Scenario (2026-2030)',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.5, label='High Risk Threshold')\n",
        "plt.colorbar(label='Fragility Probability')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('country_risk_vs_volume.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: country_risk_vs_volume.png\")\n",
        "\n",
        "# VIZ 3: Scenario Comparison by Country (Top 5 Risk Countries)\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Get top 5 riskiest countries in moderate scenario\n",
        "top_5_countries = scenario_country_risk['moderate'].head(5).index\n",
        "\n",
        "x = np.arange(len(top_5_countries))\n",
        "width = 0.25\n",
        "\n",
        "conservative_vals = [scenario_country_risk['conservative'].loc[c, 'avg_fragility_probability']\n",
        "                     for c in top_5_countries]\n",
        "moderate_vals = [scenario_country_risk['moderate'].loc[c, 'avg_fragility_probability']\n",
        "                 for c in top_5_countries]\n",
        "aggressive_vals = [scenario_country_risk['aggressive'].loc[c, 'avg_fragility_probability']\n",
        "                   for c in top_5_countries]\n",
        "\n",
        "ax.bar(x - width, conservative_vals, width, label='Conservative', color='#d62728')\n",
        "ax.bar(x, moderate_vals, width, label='Moderate', color='#ff7f0e')\n",
        "ax.bar(x + width, aggressive_vals, width, label='Aggressive', color='#2ca02c')\n",
        "\n",
        "ax.set_xlabel('Supplier Country', fontsize=12)\n",
        "ax.set_ylabel('Average Fragility Probability', fontsize=12)\n",
        "ax.set_title('Risk Reduction Through Policy Interventions\\nTop 5 Highest-Risk Countries',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(top_5_countries, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.3)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('country_risk_scenario_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: country_risk_scenario_comparison.png\")\n",
        "\n",
        "# VIZ 4: Country Import Concentration (Bar Chart - More Robust)\n",
        "print(\"\\n[VIZ 4] Creating import concentration chart...\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "top_n = 10\n",
        "top_countries_plot = country_import_share.head(top_n)\n",
        "\n",
        "# Get risk colors\n",
        "colors_list = []\n",
        "for country in top_countries_plot.index:\n",
        "    try:\n",
        "        risk = scenario_country_risk['moderate'].loc[country, 'avg_fragility_probability']\n",
        "        if risk > 0.6:\n",
        "            colors_list.append('#d62728')  # Red (high risk)\n",
        "        elif risk > 0.4:\n",
        "            colors_list.append('#ff7f0e')  # Orange (medium risk)\n",
        "        else:\n",
        "            colors_list.append('#2ca02c')  # Green (low risk)\n",
        "    except (KeyError, AttributeError):\n",
        "        colors_list.append('#7f7f7f')  # Gray (unknown)\n",
        "\n",
        "# Create horizontal bar chart\n",
        "bars = plt.barh(range(len(top_countries_plot)),\n",
        "                top_countries_plot['import_share_pct'],\n",
        "                color=colors_list,\n",
        "                edgecolor='black',\n",
        "                linewidth=1.5)\n",
        "\n",
        "plt.yticks(range(len(top_countries_plot)), top_countries_plot.index, fontsize=11)\n",
        "plt.xlabel('Import Share (%)', fontsize=12)\n",
        "plt.ylabel('Supplier Country', fontsize=12)\n",
        "plt.title('Import Concentration by Supplier Country\\n(Color indicates risk level)',\n",
        "          fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add value labels\n",
        "for i, (idx, row) in enumerate(top_countries_plot.iterrows()):\n",
        "    plt.text(row['import_share_pct'] + 0.5, i,\n",
        "             f\"{row['import_share_pct']:.1f}%\",\n",
        "             va='center', fontsize=10)\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#d62728', edgecolor='black', label='High Risk (>0.6)'),\n",
        "    Patch(facecolor='#ff7f0e', edgecolor='black', label='Medium Risk (0.4-0.6)'),\n",
        "    Patch(facecolor='#2ca02c', edgecolor='black', label='Low Risk (<0.4)')\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('country_import_concentration.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Saved: country_import_concentration.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0NxBnLQoyVs",
        "outputId": "d923f904-d543-421c-b575-c75eac3d599d"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 13: Creating Visualizations\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[VIZ 1] Creating ROC curves...\n",
            "‚úì Saved: roc_curve_comparison.png\n",
            "\n",
            "[VIZ 2] Creating SHAP importance plot...\n",
            "‚úì Saved: shap_importance_bar.png\n",
            "\n",
            "[VIZ 3] Creating detailed SHAP summary...\n",
            "‚úì Saved: shap_summary_detailed.png\n",
            "\n",
            "[VIZ 4] Creating scenario comparison heatmap...\n",
            "‚úì Saved: scenario_comparison_minerals.png\n",
            "\n",
            "[VIZ 5] Creating combined risk heatmap...\n",
            "‚úì Saved: risk_heatmap_all_scenarios.png\n",
            "\n",
            "[VIZ] Creating country risk visualizations...\n",
            "‚úì Saved: country_risk_ranking.png\n",
            "‚úì Saved: country_risk_vs_volume.png\n",
            "‚úì Saved: country_risk_scenario_comparison.png\n",
            "\n",
            "[VIZ 4] Creating import concentration chart...\n",
            "‚úì Saved: country_import_concentration.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 14: EXPORT RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 14: Exporting Results\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Export scenario predictions\n",
        "for scenario_name, scenario_df in scenario_predictions.items():\n",
        "    filename = f'predictions_{scenario_name}_2026_2030.csv'\n",
        "    scenario_df[['year', 'mineral', 'supplier_country', 'import_tonnage',\n",
        "                 'import_concentration', 'fragility_probability']].to_csv(filename, index=False)\n",
        "    print(f\"‚úì Saved: {filename}\")\n",
        "\n",
        "# Export feature importance\n",
        "feature_importance.to_csv('shap_feature_importance.csv', index=False)\n",
        "print(\"‚úì Saved: shap_feature_importance.csv\")\n",
        "\n",
        "# Export model performance metrics\n",
        "performance_summary = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'XGBoost'],\n",
        "    'CV_ROC_AUC_Mean': [rf_cv_scores.mean(), xgb_cv_scores.mean()],\n",
        "    'CV_ROC_AUC_Std': [rf_cv_scores.std(), xgb_cv_scores.std()],\n",
        "    'Test_ROC_AUC': [roc_auc_rf, roc_auc_xgb]\n",
        "})\n",
        "performance_summary.to_csv('model_performance_summary.csv', index=False)\n",
        "print(\"‚úì Saved: model_performance_summary.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuhdZ6NMo6ST",
        "outputId": "de5ed9ef-266d-4fcf-f5ef-4fe81a88af2d"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 14: Exporting Results\n",
            "--------------------------------------------------------------------------------\n",
            "‚úì Saved: predictions_conservative_2026_2030.csv\n",
            "‚úì Saved: predictions_moderate_2026_2030.csv\n",
            "‚úì Saved: predictions_aggressive_2026_2030.csv\n",
            "‚úì Saved: shap_feature_importance.csv\n",
            "‚úì Saved: model_performance_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
        "print(f\"   Best Model: {best_model_name}\")\n",
        "print(f\"   Test ROC-AUC: {best_roc_auc:.3f}\")\n",
        "print(f\"   CV ROC-AUC: {rf_cv_scores.mean() if best_model_name == 'Random Forest' else xgb_cv_scores.mean():.3f}\")\n",
        "\n",
        "print(f\"\\nüéØ TOP 3 RISK DRIVERS:\")\n",
        "for i, row in feature_importance.head(3).iterrows():\n",
        "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.3f}\")\n",
        "\n",
        "print(f\"\\nüìÅ GENERATED FILES:\")\n",
        "print(\"\\n  CSV Files:\")\n",
        "print(\"  - predictions_conservative_2026_2030.csv\")\n",
        "print(\"  - predictions_moderate_2026_2030.csv\")\n",
        "print(\"  - predictions_aggressive_2026_2030.csv\")\n",
        "print(\"  - model_performance_summary.csv\")\n",
        "print(\"  - shap_feature_importance.csv\")\n",
        "\n",
        "print(\"\\n  Visualizations:\")\n",
        "print(\"  - roc_curve_comparison.png\")\n",
        "print(\"  - shap_importance_bar.png\")\n",
        "print(\"  - shap_summary_detailed.png\")\n",
        "print(\"  - scenario_comparison_minerals.png\")\n",
        "print(\"  - risk_heatmap_all_scenarios.png\")\n",
        "\n",
        "print(\"\\nüí° NEXT STEPS:\")\n",
        "print(\"  1. Review visualizations (download PNG files)\")\n",
        "print(\"  2. Analyze scenario predictions (CSV files)\")\n",
        "print(\"  3. Identify high-risk minerals/suppliers\")\n",
        "print(\"  4. Develop policy recommendations\")\n",
        "print(\"  5. Write up results for paper\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Thank you for using the Critical Minerals ML Framework!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSuiOS8EpAd4",
        "outputId": "27239f35-c34a-444b-ecea-dbf06d482d84"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "‚úÖ ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "üìä MODEL PERFORMANCE:\n",
            "   Best Model: Random Forest\n",
            "   Test ROC-AUC: 0.705\n",
            "   CV ROC-AUC: 0.662\n",
            "\n",
            "üéØ TOP 3 RISK DRIVERS:\n",
            "   9. price_volatility: 0.060\n",
            "   10. import_value_usd: 0.036\n",
            "   1. import_concentration: 0.034\n",
            "\n",
            "üìÅ GENERATED FILES:\n",
            "\n",
            "  CSV Files:\n",
            "  - predictions_conservative_2026_2030.csv\n",
            "  - predictions_moderate_2026_2030.csv\n",
            "  - predictions_aggressive_2026_2030.csv\n",
            "  - model_performance_summary.csv\n",
            "  - shap_feature_importance.csv\n",
            "\n",
            "  Visualizations:\n",
            "  - roc_curve_comparison.png\n",
            "  - shap_importance_bar.png\n",
            "  - shap_summary_detailed.png\n",
            "  - scenario_comparison_minerals.png\n",
            "  - risk_heatmap_all_scenarios.png\n",
            "\n",
            "üí° NEXT STEPS:\n",
            "  1. Review visualizations (download PNG files)\n",
            "  2. Analyze scenario predictions (CSV files)\n",
            "  3. Identify high-risk minerals/suppliers\n",
            "  4. Develop policy recommendations\n",
            "  5. Write up results for paper\n",
            "\n",
            "================================================================================\n",
            "Thank you for using the Critical Minerals ML Framework!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY: MODEL PERFORMANCE & CONFUSION MATRIX\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ FINAL MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüìä BEST MODEL: {best_model_name}\")\n",
        "print(f\"   ROC-AUC: {best_roc_auc:.3f}\")\n",
        "\n",
        "# Get final predictions with optimal threshold\n",
        "if 'optimal_threshold' in locals():\n",
        "    # If threshold optimization was run\n",
        "    print(f\"\\nüéØ OPTIMAL THRESHOLD: {optimal_threshold:.2f}\")\n",
        "    y_pred_final = (best_model.predict_proba(X_test)[:, 1] >= optimal_threshold).astype(int)\n",
        "else:\n",
        "    # Default threshold\n",
        "    print(f\"\\nüéØ THRESHOLD: 0.50 (default)\")\n",
        "    y_pred_final = best_model.predict(X_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm_final = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "print(\"\\nüìã CONFUSION MATRIX:\")\n",
        "print(\"=\" * 80)\n",
        "print(cm_final)\n",
        "\n",
        "# Detailed breakdown\n",
        "tn, fp, fn, tp = cm_final.ravel()\n",
        "\n",
        "print(f\"\\nDETAILED BREAKDOWN:\")\n",
        "print(f\"  True Negatives (TN):  {tn:2d} ‚úì Correctly predicted stable\")\n",
        "print(f\"  False Positives (FP): {fp:2d} ‚úó False alarm (predicted disruption, was stable)\")\n",
        "print(f\"  False Negatives (FN): {fn:2d} ‚úó MISSED disruption (predicted stable, was disruption)\")\n",
        "print(f\"  True Positives (TP):  {tp:2d} ‚úì Correctly predicted disruption\")\n",
        "\n",
        "# Calculate key metrics\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"\\nüìà PERFORMANCE METRICS:\")\n",
        "print(f\"  Accuracy:  {accuracy:.1%} (correct predictions)\")\n",
        "print(f\"  Precision: {precision:.1%} (when we predict disruption, we're right {precision:.0%} of time)\")\n",
        "print(f\"  Recall:    {recall:.1%} (catching {recall:.0%} of actual disruptions)\")\n",
        "print(f\"  F1-Score:  {f1:.3f} (harmonic mean of precision & recall)\")\n",
        "\n",
        "# Critical metric for policy\n",
        "print(f\"\\n‚ö†Ô∏è  CRITICAL FOR POLICY:\")\n",
        "print(f\"  FALSE NEGATIVE RATE: {fn}/{fn+tp} = {fn/(fn+tp):.1%}\")\n",
        "print(f\"  ‚Üí We are MISSING {fn/(fn+tp):.0%} of real disruptions\")\n",
        "print(f\"  ‚Üí We are CATCHING {recall:.0%} of real disruptions ‚úì\")\n",
        "\n",
        "# Visual representation\n",
        "print(f\"\\nüìä VISUAL CONFUSION MATRIX:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"                    PREDICTED\")\n",
        "print(\"                Stable    Disruption\")\n",
        "print(\"              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "print(f\"       Stable ‚îÇ   {tn:2d}    ‚îÇ    {fp:2d}     ‚îÇ\")\n",
        "print(\"ACTUAL        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
        "print(f\"   Disruption ‚îÇ   {fn:2d}    ‚îÇ    {tp:2d}     ‚îÇ\")\n",
        "print(\"              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "\n",
        "# Comparison if threshold was optimized\n",
        "if 'optimal_threshold' in locals() and optimal_threshold != 0.5:\n",
        "    # Calculate with default threshold for comparison\n",
        "    y_pred_default = (best_model.predict_proba(X_test)[:, 1] >= 0.5).astype(int)\n",
        "    cm_default = confusion_matrix(y_test, y_pred_default)\n",
        "    tn_d, fp_d, fn_d, tp_d = cm_default.ravel()\n",
        "\n",
        "    print(f\"\\nüìä IMPROVEMENT FROM THRESHOLD OPTIMIZATION:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"  Threshold 0.50 ‚Üí {optimal_threshold:.2f}\")\n",
        "    print(f\"  False Negatives: {fn_d} ‚Üí {fn} (reduced by {fn_d - fn})\")\n",
        "    print(f\"  False Positives: {fp_d} ‚Üí {fp} (increased by {fp - fp_d})\")\n",
        "    print(f\"  Recall: {tp_d/(tp_d+fn_d):.0%} ‚Üí {recall:.0%} (improved by {(recall - tp_d/(tp_d+fn_d))*100:.0f} percentage points)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K5BCWg1rp-u",
        "outputId": "3e8440ca-eb5e-49b2-af34-68af4f6efbba"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üéØ FINAL MODEL PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "\n",
            "üìä BEST MODEL: Random Forest\n",
            "   ROC-AUC: 0.705\n",
            "\n",
            "üéØ OPTIMAL THRESHOLD: 0.30\n",
            "\n",
            "üìã CONFUSION MATRIX:\n",
            "================================================================================\n",
            "[[ 3 25]\n",
            " [ 0 24]]\n",
            "\n",
            "DETAILED BREAKDOWN:\n",
            "  True Negatives (TN):   3 ‚úì Correctly predicted stable\n",
            "  False Positives (FP): 25 ‚úó False alarm (predicted disruption, was stable)\n",
            "  False Negatives (FN):  0 ‚úó MISSED disruption (predicted stable, was disruption)\n",
            "  True Positives (TP):  24 ‚úì Correctly predicted disruption\n",
            "\n",
            "üìà PERFORMANCE METRICS:\n",
            "  Accuracy:  51.9% (correct predictions)\n",
            "  Precision: 49.0% (when we predict disruption, we're right 49% of time)\n",
            "  Recall:    100.0% (catching 100% of actual disruptions)\n",
            "  F1-Score:  0.658 (harmonic mean of precision & recall)\n",
            "\n",
            "‚ö†Ô∏è  CRITICAL FOR POLICY:\n",
            "  FALSE NEGATIVE RATE: 0/24 = 0.0%\n",
            "  ‚Üí We are MISSING 0% of real disruptions\n",
            "  ‚Üí We are CATCHING 100% of real disruptions ‚úì\n",
            "\n",
            "üìä VISUAL CONFUSION MATRIX:\n",
            "================================================================================\n",
            "                    PREDICTED\n",
            "                Stable    Disruption\n",
            "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "       Stable ‚îÇ    3    ‚îÇ    25     ‚îÇ\n",
            "ACTUAL        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "   Disruption ‚îÇ    0    ‚îÇ    24     ‚îÇ\n",
            "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "üìä IMPROVEMENT FROM THRESHOLD OPTIMIZATION:\n",
            "================================================================================\n",
            "  Threshold 0.50 ‚Üí 0.30\n",
            "  False Negatives: 10 ‚Üí 0 (reduced by 10)\n",
            "  False Positives: 11 ‚Üí 25 (increased by 14)\n",
            "  Recall: 58% ‚Üí 100% (improved by 42 percentage points)\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}